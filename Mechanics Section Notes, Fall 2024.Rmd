---
title: "Physics 151 F24 Notes"
author: "Nicholas Lyu, Arthur Jaffe"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [refs.bib]
biblio-style: "numeric"
split_bib: yes
link-citations: true
---

\usepackage{cancel}
\usepackage{amsmath}
\usepackage{bm}
\newcommand{\pd}[1]{\partial_{#1}}
\newcommand{\pb}[2]{\left[{#1}, {#2}\right]_\xi}

\newcommand{\mbb}{\mathbb}
\newcommand{\mbf}{\mathbf}
\newcommand{\mb}{\boldsymbol}
\newcommand{\mrm}{\mathrm}
\newcommand{\mca}{\mathcal}
\newcommand{\mfk}{\mathfrak}
\newcommand{\tr}{\mrm{tr}} 
\newcommand{\df}{\dfrac}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\dag}{\dagger}

\newcommand{\Cl}{\mca C}
\newcommand{\Gr}{\mca G}
\newcommand{\Pf}{\mrm{Pf}}
\newcommand{\Pa}{\mca P}
\newcommand{\R}{\mbb R}

\newcommand{\poly}{\mrm{poly}}



# Preface {-}


## Introduction to the course {-}

For math concentrators, at some point 
(late into high school or in college) there is a 
change of perspective from _computation_ to _proof_-based math. 
For example, one stops viewing matrices as large blobs of numbers but 
as coordinate representations of abstract linear transformations. 

There is a similar shift in perspective in physics, when
the emphasis changes from solving equations of motion to 
understanding the fundamental reasons they're there in 
the first place. Key to navigating this change is understanding 
the role of symmetry and conservation laws, as well as the 
importance of _operators_ in physics. 

The focus of this course is not quantum theory 
(maybe at the end, time permitting), but the 
classical treatment of symmetry and conservation 
will help motivate much of quantum theory's constructions. 

## Using these notes {-}

These notes accompany the Fall 2024 
iteration of Arthur Jaffe's _Mechanics_ course at 
Harvard (Physics 151). 
They recount main results and, occasionally, 
supplement Prof. Jaffe's lecture notes on canvas. 
It can serve as a concise reminder of the results 
in lecture. 

The main deliverables are: 

1. Covariant and extremal-action perspectives on Lagrangian mechanics. 
2. Connection between Lagrangian and Hamiltonian mechanics 
    via the Legendre transform. 
3. Nother's theorem: continuous symmetry $\implies$ conserved quantity. 
4. Spacetime geometry: unitary representation of (a component of) the Lorentz group. 

## A note on notation {-}

It is always important to be careful about 
derivative maps, where overloaded notation can frequently 
lead to confusion. We use the following standard notation for 
_partial derivatives_:  
\[ 
    \pd {x_1} f(x_1, x_2) = 
    \df{\partial}{\partial x_1} f(x_1, x_2)
\] 
For second-order derivatives, we adopt 
\[ 
    \pd {x_1, x_2}^2 f(x_1, x_2) 
    = \df{\partial^2}{\partial x_1\partial x_2} f(x_1, x_2)
\] 
We also use the following not-so-standard notation 
for _total derivatives_: 
\[ 
    d_t f(x_1(t), x_2(t)) = \df d {dt} f(x_1(t), x_2(t))
\] 
When we wish to specify the point at which a derivative is 
evaluated, we write e.g. 
\[ 
    d_t\big|_0 f(x_1(t), x_2(t)) = x'_1(0) \pd {x_1} f + x'_2(0) \pd {x_2} f
\] 

<!--chapter:end:index.Rmd-->

# Two-body problem
We approach the two-body problem in the following steps: 

1. Reduce to one-body motion. 
2. Identify the conserved quantities: energy $E$, 
angular-momentum $\mbf L$, and Runge-Lenz vector $\mb \epsilon$.
    - $E$ is conserved when the potential $V$ is time-invariant. 
    - $\mbf L$ is conserved when $V$ is central. 
    - $\mb \epsilon$ is conserved when $V\propto r^{-2}$. 
    Orbits of such potentials have conserved eccentricity. 
3. Reduce to planar motion using conservation of $\mbf L$. 
4. Derive the orbit equations from conserved quantities. 
5. Analyze the different kinds of orbits by 
    looking at the one-dimensional effective potential $V_{\mrm{eff}}$. 
6. Analyze scattering. 


## Problem setup {-}
::: {.definition #twoBodyProblem name="two-body problem"}
Consider two particles with mass $m_1, m_2$ 
at locations $\mbf x_1, \mbf x_2 \in \R^3$. 
Their potential is 
\[ 
    V(\mbf x_1, \mbf x_2) = -\df{k}{|\mbf x_1-\mbf x_2|}
\] 
Here $k\in \R$ is a constant. The interaction is attractive 
when $k>0$ and repulsive when $k<0$. 
:::
Computing the force based on the potential: 
\[ 
    \mbf F_j 
    = -\nabla_{\mbf x_j}V(\mbf x_1, \mbf x_2)
    = \df{k}{|\mbf x_1 - \mbf x_2|^2} 
    \nabla_{\mbf x_j}|\mbf x_1 - \mbf x_2|
    = \df{k}{|\mbf x_1-\mbf x_2|^2} 
    \begin{cases}
        (\mbf x_1 - \mbf x_2) & j = 1 \\ 
        (\mbf x_2 - \mbf x_1) & j = 2 
    \end{cases}
    (\#eq:twoBodyForce)
\] 
Note that $\mbf F_1 + \mbf F_2 = 0$. 
Recalling Newton's second law, the equations of motion are 
\[ 
    \ddot {\mbf x}_j = \mbf F_j / m_j 
\] 

Instead of solving for $\mbf x_1, \mbf x_2$, one 
can solve instead for the motion of the center of mass 
$\mbf R$ and displacement $\mbf r$. The motion of $\mbf R$ 
will be trivial. 

::: {.proposition name="center of mass motion"}
Define the center of mass 
\[ 
    \mbf R = \df{m_1}{m_1+m_2} \mbf x_1 + \df{m_2}{m_1+m_2}\mbf x_2 
\] 
The equation for the center of mass is 
\[ 
    (m_1+m_2)\ddot {\mbf R} = m_1\ddot{\mbf r}_1 + m_2\ddot{\mbf r}_2 
    = \mbf F_1 + \mbf F_2 = 0 
\] 
The motion of the center of mass is thus completely determined by the 
initial conditions of the problem. 
:::

::: {.definition name="relative coordinates, displacement"}
Define the relative coordinates $\mbf r_j = \mbf x_j - \mbf R$ 
and displacement $\mbf r = \mbf x_2 - \mbf x_1$. 
Denote by $r=|\mbf r|, \mbf n = \mbf r / r$. 
Note that $\mbf n$ points in the direction $1\to 2$. 
:::

::: {.proposition name="one-body equation"}
The coordinates $\mbf r$ obeys 
\[ 
    \ddot {\mbf r} = \mbf F/\mu, 
    \quad \mbf F = -\nabla_{\mbf r} V(r) = -\df{k}{r^3} \mbf r
    \quad \mu = \df{m_1m_2}{m_1+m_2}
\] 
Here $\mu$ is the reduced mass which satisfies $1/m_1 + 1/m_2 = 1/\mu$. 
It is the "effective" mass of the one-body "particle" 
corresponding to the two-body problem. 
Additionally define the one-body momentum 
\[ 
    \mbf p = \mu \dot {\mbf r}, \quad \dot {\mbf p} = \mbf F 
\] 

_Proof:_ Rewriting \@ref(eq:twoBodyForce) in terms of the 
newly defined quantities: 
\begin{align} 
    m_1 \ddot {\mbf x}_1 
    &= \df{k}{r^2}\mbf n 
    = -m_2 \ddot {\mbf x}_2 \\ 
    \ddot {\mbf r} 
    &= \ddot {\mbf x}_2 - \ddot {\mbf x}_1
    = -\left(\df 1 {m_2} + \df 1 {m_1}\right)\df{k}{r^2} \mbf n 
    = - \df{k}{\mu r^2}\mbf n 
\end{align}
:::

## Three conserved quantities {-}
::: {.definition name="conserved quantity (non-relativistic)"}
In Newtonian mechanics, a quantity is ''conserved'' if 
it remains constant under time-evolution. 
:::

::: {.proposition name="conservation of energy"}
The energy scalar 
\[ 
    E = T + V = \df{\mbf p^2}{2\mu} + V 
\] 
is conserved by Newton's equation of motion 
$\mu \ddot {\mbf r} = \mbf F = -\nabla_{\mbf r} V(r)$ 

_Proof:_ Direct computation 
\[ 
    d_t E = d_t \left(
        \df{\mu^2 \dot {\mbf r} \cdot \dot {\mbf r}}{2\mu} + V
    \right) 
    = \mu \dot {\mbf r}\cdot \ddot {\mbf r} 
    - (\nabla_{\mbf r} V) \cdot \mbf r = 0 
\] 
Note that $d_tV$ is computed according to 
the dependence $V\leftarrow \mbf r\leftarrow t$. 
:::

::: {.proposition name="conservation of angular momentum"}
The angular momentum vector 
\[ 
    \mbf L = \mbf r\times \mbf p 
\] 
Is conserved for any _central force problem_: one in which 
$V(\mbf r)=V(r, \mbf n) = V(r)$ is only dependent on the 
magnitude, but not direction, of $r$. 

_Proof:_ Direct computation 
\[ 
    d_t \mbf L 
    = d_t(\mbf r\times \mbf p)
    = \dot {\mbf r}\times \mbf p + \mbf r \times \mbf F
    = 0 + 0 
\] 
The first term vanishes by 
$\mbf p \parallel \dot {\mbf r}$ and the second by 
the definition of central potential. 
:::

::: {.remark}
This is our first example of symmetry-conservation. 
A central force problem demonstrates **spherical symmetry**. 
A rigorous definition of "symmetry" will be given soon. 
:::

::: {.proposition name="planar reduction"}
The trajectory $\mbf r$ lie in the plane 
orthogonal to $\mbf L$. Given this, let $\mbf n =\mbf r/r$ 
be the first unit component of the plane and the second 
\[ 
    \mbf l = (\mbf L \times \mbf r) / |\mbf L\times \mbf r| 
\] 
In polar coordinates, 
\[ 
    \dot {\mbf r} = \dot r \mbf n + r\dot \theta \mbf l 
    \implies \mbf L = \mu r^2 \dot \theta (\mbf n\times \mbf l)
\] 

_Proof:_ $\mbf L = \mu \, \mbf r \times \dot {\mbf r}$ 
is orthogonal to both $\mbf r$ and $\dot {\mbf r}$, and 
$\mbf L$ is conserved. 
:::

::: {.proposition #rungeLenzDefinition name="conservation of the Runge-Lenz vector"}
The Runge-Lenz vector $\mb \epsilon$ is defined as 
\[ 
    \mb \epsilon 
    = \df 1 {\mu k} \mbf p \times \mbf L - \mbf n 
    = \df 1 {\mu k} [\mbf p \times (\mbf r\times \mbf p)] - \mbf n 
    = \df 1 {\mu k} [\mbf p \times (\mbf r\times \mbf p)] - \mbf n 
    = \df {pL} {\mu k} \mbf n - \mbf n 
\] 
The vector is conserved. 

_Proof:_ Direct computation. 
\begin{align}
    \dot {\mb \epsilon} 
    &= \df 1 {\mu k} \mbf F \times \mbf L - \dot {\mbf n} 
    = \df 1 {\mu k} \left(-\df{k}{r^2} \mbf n\right) \times 
    [\mu r^2 \dot \theta (\mbf n \times \mbf l)] - \dot {\mbf n} \\ 
    &= -\dot \theta [\mbf n\times (\mbf n \times \mbf l)] - \dot {\mbf n}
    = \dot \theta \mbf l - \dot {\mbf n} = 0 
\end{align}
:::

::: {.proposition #rungeLenzMagnitude name="magnitude of the Runge-Lenz vector"}
The magnitude $\epsilon = |\mb \epsilon|$ is 
\[ 
    \epsilon^2 = 1 + \df{2EL^2}{\mu k^2}
\] 
_Proof:_ First compute 
$\mbf p\times \mbf L = \mbf p \times (\mbf r \times \mbf p) = Lp\mbf n$. 
Then $\mbf n \cdot (\mbf p\times \mbf L) = Lp$ and 
$|\mbf p\times \mbf L|^2 = p^2L^2$ since $\mbf p \perp \mbf L$. 
Also note that $p/L = 1/r$, then 
then 
\begin{align}
    \epsilon^2 
    &= \mbf n\cdot \mbf n  + \df{1}{\mu^2 k^2}
    [(\mbf p\times \mbf L)\cdot (\mbf p\times \mbf L)]
    - \df{2}{\mu k} \mbf n \cdot (\mbf p\times \mbf L) \\ 
    &= 1 + \df{p^2 L^2}{\mu^2k^2} - 2 \df{pL}{\mu k} 
    = 1 + \df{2L^2}{\mu k^2}\left(\df{p^2}{2\mu} - \df{pk}{L}\right)
    = 1 + \df{2EL^2}{\mu k^2}
\end{align}
:::

## Orbits, effective potential {-}
We can obtain an implicit equation of motion using the conservation equations. 
By definition of $\mb \epsilon$ \@ref(prp:rungeLenzDefinition)
\[ 
    \mbf r\cdot \mb \epsilon 
    = \df{rpL}{\mu k} - r = \df{L^2}{\mu k} - r
\] 
Let $\mbf r\cdot \mb \epsilon = r\epsilon \cos\theta$, 
then the orbit equation reads 
\[ 
    r(1+\cos\theta) = \df{L^2}{\mu k}
     (\#eq:orbitEquation)
\] 

::: {.remark}
Here the orientation is such that $\theta=0$ points along $\mb \epsilon$. 
This is also an _implicit_ orbit equation since time is not an explicit 
variable.  
:::
The lecture note appendix works out the conic section 
trajectories corresponding to different ranges of $\epsilon$. 

::: {.definition #effectivePotential name="effective radial potential"}
So far, the kinetic energy is computed according to the vector derivative 
$\dot {\mbf r}$ 
\[ 
    E = T + V = \df 1 2 \mu \dot {\mbf r}^2 - \df{k}{r}
\] 
In our case, we can reduce this to a scalar problem 
by introducing the effective potential. 
Recall $\dot {\mbf r} = \dot (r\mbf n) 
= \dot r\mbf n + r\dot {\mbf n} = \dot r\mbf n + r\dot \theta \mbf l 
\implies \dot {\mbf r}^2 = \dot r^2 + r^2\dot \theta^2 = \dot r^2 + L^2/\mu^2 r^2$ 
\[ 
    E = \df 1 2 \mu \dot{\mbf r}^2 - \df k r 
    = \df 1 2 \mu \dot r^2 + \left(\df {L^2}{2\mu r^2} - \df k r \right)
\] 
The last term in paranthesis is called the "effective potential $V_{\mrm{eff}}$. 
:::

::: {.remark}
Recall that $L$ is conserved when the potential displays spherical symmetry. 
The effective potential conveniently reduces a _vector problem_ 
into a _scalar problem_ at the cost of introducing a "centrifugal" term. 
This is a prime example of symmetry helping simplify analysis. 
The effective potential reduction will show up again in the 
QM treatment of the hydrogen atom (Physics 143a) and spherical gravity 
correction (Physics 210). 
:::
Energy is conserved along orbits, and we can easily identify the 
bound, unbound, and spherical orbits from the effective-potential graph. 

::: {.exercise name="relativistic gravitational effective-potential"}
Identify the effective potential for Newtonian gravity, where $k=GM$. 
What is the radius of the circular orbit? 

The effective radial potential from a spherical mass according to 
general relativity is 
\[ 
    V_{\mrm{eff}} = -\sigma \df{GM}{r} + \df{L^2}{2r^2} - \df{GML^2}{r^3}
\] 
Where $\sigma=0$ if the particle is massless else $1$. 
What are the behaviors for massive and massless particles? 
:::


## Scattering {-}
In scattering problems, we assume a spatially and temporally uniform distribution 
of incoming beams of particles along the incident $z$-axis. 
We use cylindrical coordinates $(z, b, \phi)$ denoting 
height, radius, and the azimuthal angle, respectively. The main quantities are: 

1. Impact parameter $b$, scattering angle $\theta$. 
2. Particles are incident within an infinitesimal patch with cross-sectional 
    area $d\sigma(b, \phi) = b\, db\, d\phi$. 
3. Particle emission are considered according to a solid angle 
    $d\Omega(\theta, \phi) = \sin\theta \, d\theta\, d\varphi$. 
4. The differential cross-section $d_\Omega\sigma \equiv \df{b}{\sin\theta}|d_\theta b|$. 
    Usually, the greater $\theta$ (heavier scattering effect), the smaller $b$, since 
    incident particles shoot closer to the scattering source -- thus the absolute sign. 
5. The total cross-section $\sigma = \int (d_\Omega \sigma)\, d\Omega$. This is the total 
    cross-sectional area which encounters scattering. For classical hard-sphere scattering, 
    this is $\pi R^2$. 

```{r echo=FALSE}
knitr::include_graphics(rep("images/scattering.jpeg", 1))
```

::: {.remark}
When the scattering source is spherically symmetric, 
all dependences on $\phi$ can be dropped. 

The differential cross-section $d_\Omega \sigma$ asks: 
at angle $\theta$ from scattering center, what is the 
impact parameter $b$ and how much unit increase in 
output solid angle will be able to account for unit 
area increase in the incident beams? 
:::

To compute the two-body scattering differential cross-section, 
the first step is deriving the relation between the impact 
parameter $b$ and scattering angle $\theta$. 

::: {.proposition #scatteringRelation name="scattering relation"}
$b(\theta) = \df{|k|}{2E} \cot(\theta/2)$

_Proof:_ Recall the orbit equation \@ref(eq:orbitEquation): 
denote the planar angle $\phi$, 
let $D = \df{L^2}{\mu k}$, and expand in terms of $x, y$
\[ 
    r(1 + \epsilon \cos\phi) = r + \epsilon x = D
\] 
Expanding in terms of $x, y$ 
\begin{align}
    r^2 &= x^2 + y^2 = (D - \epsilon x)^2 \\ 
    y^2 &= (\epsilon^2 - 1)x^2 - 2D\epsilon x + D^2
\end{align}
For repulsive orbits, $\epsilon>1$ and the orbits asymptote to 
\[ 
    y \sim \pm \sqrt{\epsilon^2 - 1} x, \quad |x|\to \infty 
\] 
The scattering angle $\theta$ in the scattering diagram thus satisfy 
\begin{align}
    \tan\theta_{\mrm{max}} = \sqrt{\epsilon^2 - 1}, 
    \quad \tan(\theta/2) = \tan(\pi/2 - \theta_{\mrm{max}}) 
    = \cot(\theta_{\mrm{max}})
\end{align}
```{r echo=FALSE}
knitr::include_graphics(rep("images/rutherford_scattering.jpeg", 1))
```
The angular momentum $L$ of the incoming particle can be 
computed at the incident limit: 
\begin{align}
    L &= |\mbf r\times \mbf p| = b p = b\sqrt{2\mu E} \\ 
    \cot(\theta/2) &= \tan(\theta_{\mrm{max}}) 
    = \sqrt{\df{2EL^2}{\mu k^2}} = \df{2bE}{|k|}
\end{align}
:::

::: {.theorem name="Rutherford cross-section formula"}
For repulsive two-body scattering, the cross-section is dependent upon $\theta$ as 
\[ 
    (d_\Omega \sigma)(\theta) = \left(\df{k}{4E}\right)^2 \sin^{-4}\df{\theta}{2}
\] 

_Proof:_ The first step is elucidating the dependence 
$d_\Omega \sigma \leftarrow b,\, d_\theta b \leftarrow \theta$. 
From \@ref(prp:scatteringRelation) we have 
\begin{align}
    b(\theta) &= \df{|k|}{2E}\cot(\theta/2)  \\ 
    d_\theta b &= -\df{|k|}{4E} \mrm{csc}^2(\theta/2)
\end{align}
Substituting into the relation $d_\Omega \sigma = b|d_\theta b|/\sin\theta$ yields 
formula as claimed. 
:::

<!--chapter:end:01-two-body-problem.Rmd-->

# Lagrange's Equations

In this section we develop two perspectives on Lagrange's equations: 

1. A "covariant" formulation of Newton Cartesian equations. 
2. Equation of the trajectory satisfying the principle of least action. 

We note in the subfinal section that the two perspectives are related: 
<span style="color:green">
the Euler-Lagrange vector $\mbb L_q$ is (negative) the representing 
element of the directional derivative map 
$\eta \mapsto (D_\eta \mca S)(q)$, which is a linear functional 
in the endpoint-preserving perturbation $\eta$. 
</span>

Both perspectives are extremely important to the 
development of modern theoretical physics. 
Computationally, make sure to understand lemma \@ref(lem:calcVariation), 
which appears again in the proof of Noether's theorem. 

## Covariant formulation {-}

### Covariance: introduction {-}
What do we mean by something covariant? 

Consider a function $f:\R^2\to \R$ defined on the 2D plane according to 
\[ 
    f_1(x, y) = x^2+y^2 
\] 
This function is, in a sense, "equivalent" to the definition 
\[ 
    f_2(r, \theta) = r^2 
\] 
Although $f_1, f_2$ are defined on different domains, 
they represent the same function $f$ defined on the $2D$ plane: 
the choice of $(x, y)$ versus $(r, \theta)$ are simply different 
choices of "charts" to represent the same underlying 
geometric object (manifold). When we specify $f(r, \theta)=r^2$ and 
understand $(r, \theta)$ as a chart for $\R^2$, 
we have equivalently defined $f(x, y)=x^2+y^2$. 
In this sense, $f$ is _covariant_. 

::: {.remark}
Covariant objects are, in a sense, analogous to "platonic ideas." 
Another prime example of a covariant object is an abstract linear 
transformation (e.g. rotation): in this case the matrix representations 
are obtained after fixing a chart (basis). 
:::

::: {.remark}
If you want to dig deeper, study differential geometry!
:::

### Euler-Lagrange Equations {-}
Let $(q_j)=\{q_1, \cdots, q_N\}$ denote the degrees of freedom. 
A trajectory's "snapshot" at a single time is completely 
captured by $((q_j), (\dot q_j))$. A Lagrangian is a scalar 
(function) which looks at such snapshots and evaluates to a number. 

:::{.definition #lagrangian name="Lagrangian"}
The Lagrangian of a physical system is a map of type 
\[  
    \mca L: \R^n\times \R^n \times \R\to \R 
\] 
Its arguments are abbreviated $\mca L(q, \dot q, t)$ but expands to 
\[ 
    \mca L(q_1, \cdots, q_n, \dot q_1, \cdots, \dot q_n, t)
\] 
:::
To be precise, the Lagrangian is a function on the _tangent bundle_ $TM$
of the configuration manifold $TM$. 

:::{.definition #eulerLagrange name="Euler-Lagrange Equations"}
According to the EL-equations, a trajectory $q(t)$ is physical if 
\[ 
    0 = \mbb L_q(t) = (d_t \nabla_{\dot q} - \nabla_q)\mca L(q, \dot q, t) \in \R^n 
\] 
for all $t$. The notation above unpacks to $\forall j=1, \cdots, n$. 
\[ 
    d_t (\pd {\dot q_j} \mca L) = \pd {q_j} \mca L
\] 
the time-dependent Euler-Lagrange vector $\mbb L_q$ 
captures the deviation from $0$; 
we use the subscript to denote explicit dependence on the coordinates $q$. 
:::
In Cartesian coordinates, define the Newtonian Lagrangian to be 
\[ 
    \mca L(x, \dot x, t) = T - V = \df m 2 \dot q^2 - V(q, t) 
\] 
Then Lagrange's equations (definition \@ref(def:eulerLagrange)) 
simplify exactly to Newton's equations. 
\[ 
    m\ddot x = \nabla_x V
    (\#eq:NewtonEquations)
\] 

### Covariance of Euler-Lagrange Equations {-}
The following theorem establishes that the following statements are equivalent: 

1. Newton's equations \@ref(eq:NewtonEquations) are true in Cartesian 
coordinates
2. Lagrange's equations are true in arbitrary invertible coordinates. 


:::{.theorem #covarianceEL name="Covariance of Euler-Lagrange equations"}
Given a coordinate transform $x\to q$, the EL-vector transforms covariantly 
according to 
\[ 
    \mbb L_q(t) = J_{q\to x} \mbb L_x(t) 
\] 
:::

To prove this, we need to establish several lemmas. 
We begin by looking at how the change of coordinates $q\to x$ 
determines $(q, \dot q)\to (x, \dot x)$. 

:::{.lemma #dotCancellation name="dot cancellation"}
$J_{\dot q\to \dot x} = J_{q\to x}$. 
:::
_Proof:_ Follows from chain rule: 
\begin{align}
    \dot x_j 
    &= d_t x_j(q_1, \cdots, q_j) 
    = (\pd {q_k} x_j) \dot q_k \implies 
    \dot x = J_{q\to x} \dot q
\end{align}
Now, $J_{q\to x}$ is independent of $\dot q$, so 
$J_{\dot q\to \dot x} = J_{q\to x}$. 

:::{.lemma #absorbDer}
$J_{q\to \dot x} = d_t J_{q\to x}$ 
:::
_Proof:_ 
On the LHS, $\pd {q_i} \dot x_j = \pd {q_i} (\dot q_k \pd {q_k} x_j)
= \dot q_k (\pd {q_iq_k}^2 x_j)$. 
On the RHS, $(d_t J_{q\to x}(q_1, \cdots, q_n))_{ij} = (\pd {q_i q_k}^2 x_j) \dot q_k$. 

The next two lemmas essentially restate the chain rule for $q(x)$ 
and $\dot q(x, \dot x)$. 

:::{.lemma #gradient1}
$\nabla_{\dot x} = J_{\dot q\to \dot x} \nabla_{\dot q}$. 
:::

:::{.lemma #gradient2}
$\nabla_x = J_{\dot q\to x} \nabla_q + J_{q\to x}\nabla_x$. 
:::

We are now ready to prove theorem \@ref(thm:covarianceEL)
by expanding the operator $d_t\nabla_{\dot x} - \nabla_x$: 
\begin{align}
    d_t \nabla_{\dot x} - \nabla_x 
    &= d_t (J_{\dot q\to \dot x} \nabla_{\dot q}) 
    - J_{q\to x}\nabla_q - J_{x\to \dot q}\nabla_{\dot q} \\ 
    &= (d_t J_{\dot q\to \dot x})\nabla_{\dot q} 
    + J_{\dot q\to \dot x} d_t\nabla_{\dot q}
    - J_{q\to x}\nabla_q - J_{q\to \dot x}\nabla_{\dot q} \\ 
    &= (d_tJ_{x\to q})\nabla_{\dot q} 
    + J_{q\to x} d_t\nabla_{\dot q}
    - J_{q\to x}\nabla_q - J_{q\to \dot x}\nabla_{\dot q} \\ 
    &= J_{q\to \dot x}\nabla_{\dot q} 
    + J_{q\to x} d_t\nabla_{\dot q}
    - J_{q\to x}\nabla_q - J_{q\to \dot x}\nabla_{\dot q} \\ 
    &= J_{q\to x} d_t\nabla_{\dot q}
    - J_{q\to x}\nabla_q \\ 
    &= J_{q\to x}(d_t \nabla_{\dot q} - \nabla_q)
\end{align}
We applied lemmas \@ref(lem:gradient1) and \@ref(lem:gradient2) on line $1$, 
applied the product rule to $J_{\dot x\to \dot q}\nabla_{\dot q}$ to obtain line $2$
line $2$, then cancelled the dots by lemma \@ref(lem:dotCancellation) 
to obtain line $3$. Finally, we applied lemma \@ref(lem:absorbDer) to 
line $3$, cancelled terms, and regrouped. 


### Constraints {-}
Sometimes it is convenient to consider an over-parameterized 
change of coordinates $x\to q$. In this case, 
$J_{q\to x}$ is "lean and tall", while $J_{x\to q}$ "short and wide." 
Recall the covariant formula 
\begin{align}
    \mbb L_x &= J_{x\to q} \mbb L_q 
\end{align}
When $J_{x\to q}$ is full rank, the physicality condition 
\[ 
    \mbb L_x = 0 \iff \mbb L_q=0 
\] 
When $q$ is overparameterized, however, requiring 
$\mbb L_q=0$ is too strong: the correct condition is 
\[ 
    \mbb L_x = 0 \iff \mbb L_q\in \ker J_{x\to q}
\] 

Consider a single constraint enforced by some constraint function $f(q)=0$ of signature 
$f:\R^n\to \R$. 

:::{.proposition}
Given a constraint $f:\R^N\to \R$ and a trajectory $q(t)$, 
if $f(q(t))=0$ for all points on the trajectory, then 
\[ 
    \nabla f \in \ker J_{x\to q}
\] 
To be precise, the gradient $(\nabla f)(q(t))$ evaluated 
at all points $q(t)$ for all $t$ is in the kernel of $J_{x\to q}(q(t))$ 
evaluated at $q(t)$. 
:::
_Proof:_ Recognizing the chain rule: 
$J_{x\to q} \nabla\big|_q f = J_{x\to f}=0$ since $f$ is constant. 

:::{.definition name="complete, independent constraints"}
Given an over-parameterized system with configuration in $\R^{m>n}$, where 
there are only $n$ true degrees of freedom, a set 
of constraints $\{f_j:\R^m\to \R\}_{j=1}^k$ is independent 
if for all possible configurations $q\in \R^m$, the set of vectors 
\[ 
    C(q) = \{\nabla f_j\big|_q\}_{j=1}^k 
\] 
are linearly independent. They are complete if $C(q)$ spans $\ker J_{x\to q}(q)$ 
for all points $q$. 
:::
All of this is a fancy way of saying that $f$ is complete and independent 
iff they provide necessary and sufficient information to discern physicality 
$\mbb L_q\in \ker J_{x\to q}$. Given these constraints, $\mbb L_x=0$ iff 
\begin{align}
    \mbb L_q(q, \dot q) &= \sum_j \lambda_j \nabla_q f_j(q) \in \ker J_{x\to q}, 
        \quad \lambda_j \in \R  \\ 
    f_j(q) &= 0 
\end{align}
This constrained problem can be solved by the 
standard Lagrange multiplier method. 


## Extremization formulation {-}
In this subsection, we show that the Euler-Lagrange 
equations are fulfilled if and only if a certain action functional 
is extremized. This yields a powerful variational perspective on 
physicality. 

:::{.definition #actionFunctional name="action functional"}
Given a Lagrangian $\mca L(q, \dot q, t)$ (definition \@ref(def:lagrangian)), 
its associated action functional $\mca S_{[a, b]}(p)$ takes in $3$ arguments: 

1. Begining time $a\in \R$. 
2. End time $b\in \R$. 
3. A path $q:[a, b]\to \R^n$. 

and outputs the scalar 
\[ 
    \mca S_{[a, b]}(p) = \int_a^b \mca L(q(t), \dot q(t), t)\, dt 
\] 
:::

To properly define extremization, we need the directional derivative. 
Fixing $a, b$, the directional derivative $(D_\eta \mca S_{[a, b]})(q)$ 
tells us how much $\mca S(q)$ changes if we nudge $q$ in the direction of 
$\eta$ infinitesimally. 

:::{.definition name="directional derivative of action"}
Fixing $a, b$, the directional derivative 
$(D_\eta \mca S_{[a, b]})(q)$ takes in two arguments: 

1. The perturbing path $\eta: [a, b]\to \R^n$. 
2. The path at which the perturbation is evaluated $q:[a, b]\to \R^n$. 

and outputs the scalar slope of $\mca S_{[a, b]}(q+\epsilon \eta)$ w.r.t $\epsilon$, 
evaluated at $0$: 
\[ 
    (D_\eta \mca S_{[a, b]})(q) = \lim_{\epsilon \to 0} 
    \df{\mca S_{[a, b]}(q) + \mca S_{[a, b]}(q+\epsilon \eta)}{\epsilon}
\] 
To reduce notation clutter, we abbreviate this as 
\[ 
    D_\eta S(q) = d_\epsilon \big|_0 S(q+\epsilon \eta) 
\] 
:::

The magic in the air is that the Euler-Lagrange vector 
(definition \@ref(def:eulerLagrange)) $\mbb L_q$ 
gives us the directional derivative when the input perturbation 
is endpoint-preserving. 

:::{.theorem #directionalDerivative name="endpoint-preserving derivative"}
For endpoint-preserving $\eta:[a, b]\to \R$ such that 
$\eta(a)=\eta(b)=0$, the directional derivative of $\mca S$ is given by 
\[ 
    (D_\eta \mca S)(q) = -\int_a^b \mbb L_q(t) \eta(t)\, dt 
\] 
:::
_Proof:_ Fixing $q, \eta$, for any $\epsilon>0$ let 
$q_\epsilon:[a, b]\to \R^n$ denote the family of paths given by 
\[ 
    q_\epsilon(t) = q(t) + \epsilon \eta(t)
    (\#eq:pathFamily)
\] 
Slide $d_\epsilon$ into the integral and apply lemma 
\@ref(lem:calcVariation), then apply the endpoint-vanishing condition 
\begin{align}
    (D_\eta \mca S)(q)
    &= d_\epsilon \big|_{\epsilon=0} \mca S(q+\epsilon \eta) 
    = \int_a^b d_\epsilon \mca L(q_\epsilon, \dot q_\epsilon, t)\, dt \\ 
    &= \int_a^b \left[
        -\eta \cdot \mbb L_q + d_t \left(\eta \cdot \nabla_{\dot q}\mca L\right)
    \right]\, dt \\ 
    &= \left(\eta \cdot \nabla_{\dot q}\mca L\right)\big|^b_a 
    -\int_a^b \eta \cdot \mbb L_q\, dt 
    = -\int_a^b \eta \cdot \mbb L_q\, dt
\end{align}

<span style="color:green">
The following lemma is the key component in the calculus 
of variations. </span> Make sure to read it carefully. 

:::{.lemma #calcVariation name="ε-derivative of the perturbed Lagrangian"}
With $q_\epsilon=q + \epsilon \eta$ as in \@ref(eq:pathFamily) 
\[ 
    d_\epsilon \mca L(q_\epsilon, \dot q_\epsilon) 
    = -\eta \cdot \mbb L_q + d_t \left(\eta \cdot \nabla_{\dot q}\mca L\right)
\] 
:::
_Proof:_ Using the dependence 
$\epsilon \to (q_\epsilon, \dot q_\epsilon) \to \mca L$, apply the chain rule 
\begin{align}
    d_\epsilon \mca L(q_\epsilon, \dot q_\epsilon) 
    &= (d_\epsilon q_\epsilon) \cdot (\nabla_q \mca L) + 
    (d_\epsilon \dot q_\epsilon) \cdot (\nabla_{\dot q} \mca L) \\ 
    &= \eta \cdot \nabla_q \mca L + d_t \eta \cdot \nabla_{\dot q} \mca L \\ 
    &= \eta \cdot \nabla_q \mca L 
    + d_t \left(\eta \cdot \nabla_{\dot q}\mca L \right) 
    - \eta \cdot d_t \nabla_{\dot q}\mca L \\ 
    &= \eta \cdot (\nabla_q - d_t \nabla_{\dot q})\mca L     
    + d_t \left(\eta \cdot \nabla_{\dot q} \mca L\right)\\ 
    &= -\eta \cdot \mbb L_q + d_t 
    \left(\eta \cdot \nabla_{\dot q}\mca L\right)
\end{align}
On the second line, we used integration by parts on the time-derivative 
to transfer the time-derivative on $\eta$ (which we don't like) 
to $\nabla_{\dot q}\mca L$ (which we like, since it appears in $\mbb L_q$). 

It is a standard result in real analysis (fundamental lemma of the 
calculus of variations) that for 
\[ 
    (D_\eta \mca S)(q) = -\int_a^b \mbb L_q(t) \eta(t)\, dt 
\] 
to be $0$ for all reasonable endpoint-preserving perturbations $\eta$ 
(which includes, in particular, compactly supported smoth functions), 
$\mbb L_q(t)$ must be $0$. This allows us to prove another 
equivalence result. 

:::{.theorem name="Hamilton's principle"}
Recall that, given a Lagrangian $\mca L(q, \dot q, \epsilon)$, 
we call a path $q:[a, b]\to \R^n$ physical iff 
\[ 
    \mbb L_q(t)=0, \quad t\in [a, b], \quad 
    \mbb L_q(t) = (d_t \nabla_{\dot q} - \nabla_q) \mca L(q(t), \dot q(t), t)
\] 
Then a path $q$ is physical iff it extremizes the action functional 
at $\mca S_{[a, b]}(q)$ (definition \@ref(def:actionFunctional)) 
w.r.t all endpoint-preserving perturbations 
$\eta:[a, b]\to \R^n, \eta(a)=\eta(b)=0$ 
\[ 
    (D_\eta \mca S)(q) = 0
\] 
:::

## Representation perspective {-}

In this section we give some intuition to the equation 
\[ 
    (D_\eta \mca S)(q) = -\la \mbb L_q, \eta\ra, \quad 
    \la f, g\ra = \int_a^b f(t)g(t)\, dt 
    (\#eq:actionLagrangianEquation)
\] 

To motivate this, consider a finite-dimensional real 
vector space $V$ equipped with an inner product 
$\la a, b\ra$. A (real) **linear functional** $\alpha$ 
on the space of $V$ is a function of the signature 
$\alpha:V\to \R$ satisfying linearity: 
\[ 
    \alpha(u + c v) = \alpha(u) + c\alpha(v), \quad \forall u, v\in V, \quad 
    c\in \R 
\] 
Some thought shows that every linear functional 
is uniquely specified by another vector $a\in V$ according to 
$\alpha(v) = \la a, v\ra$. Fix any basis $\{b_1, \cdots b_n\}$,
given $\alpha:V\to \R$ we let 
\[ 
    a = \sum_{j=1}^n \alpha(b_j) b_j 
    \implies 
    \la a, \sum_{j=1}^N c_j b_j\ra = \sum c_j \alpha(b_j) = 
    \alpha\left(
    \sum_{j=1}^N c_j b_j
    \right)
\] 
Fixing an interval $a, b$ and classical trajectory $q$ on $a, b$, 
note the following facts: 

1. the set of all endpoint-preserving 
    perturbations $\eta:[a, b]\to \R^n$ form a vector space $V$. 
2. The map $\eta\mapsto (D_\eta \mca S)(q)$ is a 
    linear functional w.r.t. $\eta$: this is the main content of 
    theorem \@ref(thm:directionalDerivative). 
With regards to our previous discussion, 
theorem \@ref(thm:directionalDerivative) equivalently states that we can regard 
\[ 
    -\mbb L_q = (\nabla_q - d_t \nabla_{\dot q})\mca L(q, \dot q) 
\] 
as the representation of the linear functional $\eta \mapsto (D_\eta \mca S)(q)$, 
just as how $a\in V$ represents $\alpha:V\to \R$. 

## Separability of quadratic Lagrangians {-}

The following result is also used in the 
path integral formulation of quantum mechanics. 

:::{.proposition name="Actions of quadratic Lagrangians are separable about 
the classical trajectory"}
Given a quadratic Lagrangian 
\[ 
    \mca L(q, \dot q) = u\, \dot q^2 + v\, q^2
\] 
for path $q$ wich satisfies the Euler-Lagrange equations, 
the Lagrangian satisfies 
\[ 
    \mca L(q+\eta, \dot q + \dot \eta) 
    = \mca L(q, \dot q) + \mca L(\eta, \dot \eta)
    = \mca L(q, \dot q) + \epsilon \mca L(\eta, \dot \eta) + 
    d_t \left[\eta \cdot \nabla_{\dot q}\mca L(q, \dot q)\right]
\] 
In particular, if the perturbation $\eta:[a, b]\to \R^n$ 
is additionally endpoint preserving so $\eta(a)=\eta(b)=0$, then 
\[ 
    \mca S(q+\eta) = \mca S(q)+\mca S(\eta)
\] 
:::
_Proof:_ Consider the function in $\epsilon:\R\to \R$ defined by 
\begin{align} 
    \mca L(\epsilon) 
    &= \mca L(q + \epsilon \eta, \dot q + \epsilon \dot \eta)
    = u (\dot q + \epsilon \dot \eta)^2 + v(q + \epsilon \eta)^2 \\ 
\end{align}
It helps at this point to compute some of its derivatives 
\begin{align}
    \eta \cdot \nabla_q \mca L 
    &=  \eta \cdot 2v\epsilon (q + \epsilon \eta), \quad 
    \eta \cdot \nabla_{\dot q} \mca L 
    =  \dot \eta \cdot 2u\epsilon (\dot q + \epsilon \dot \eta) \\ 
    \df 1 2 (\eta \cdot \nabla_q)^2 \mca L 
    &= (\eta \cdot \nabla_q)\left[
        \eta \cdot 2v\epsilon (q + \epsilon \eta)
    \right]
    = \epsilon v \, \eta^2 \\ 
    \df 1 2 (\eta \cdot \nabla_{\dot q})^2 \mca L 
    &= (\dot \eta \cdot \nabla_{\dot q}) 
    \left[\dot \eta \cdot 2u\epsilon (\dot q + \epsilon \dot \eta) \right]
    = \epsilon u \, \dot \eta^2 \\ 
    [(\nabla \cdot \nabla_q)(\dot \eta \cdot \nabla_{\dot q})] \mca L 
    &= [(\dot \eta \cdot \nabla_{\dot q})(\nabla \cdot \nabla_q)] \mca L = 0 \\ 
    \df 1 2 (\eta \cdot \nabla_q + \dot \eta \cdot \nabla_{\dot q})^2 \mca L(\epsilon)
    &= \epsilon \mca L(\eta, \dot \eta)
\end{align}
Taylor expanding about $\epsilon=0$ yields 
\begin{align}
    \mca L(q + \epsilon \eta, \dot q + \epsilon \dot \eta)
    &= \mca L(q, \dot q) 
        + \sum_{k=1}^\infty 
            \df {
                (\eta \cdot \nabla_q + \dot \eta \cdot \nabla_{\dot q})^k 
            } {k!} 
        \mca L(q + \epsilon \eta, \dot q + \epsilon \dot \eta) \\ 
    &= \mca L(0) 
    + (\eta \cdot \nabla_q + \dot \eta \cdot \nabla_{\dot q}) \mca L
    + \df 1 2 (\eta \cdot \nabla_q + \dot \eta \cdot \nabla_{\dot q})^2 \mca L \\ 
    &= \mca L(q, \dot q) + \epsilon \mca L(\eta, \dot \eta) 
    + (\eta \cdot \nabla_q + \dot \eta \cdot \nabla_{\dot q}) \mca L
    (\#eq:separationEquation)
\end{align}
We will now show that the extra last term (which came from the linear expansion)
is zero when $\eta$ is time-dependent. We do this by using the product 
rule and the condition that at $\epsilon=0$, the path satisfies the Euler-Lagrange equation 
\begin{align} 
    (\eta \cdot \nabla_q + \dot \eta \cdot \nabla_{\dot q})\mca L
    &= [\eta \cdot \nabla_q + d_t(\eta \cdot \nabla_{\dot q}) - 
    \eta \cdot d_t \nabla_{\dot q}] \mca L \\ 
    &= d_t(\eta \cdot \nabla_{\dot q})\mca L
\end{align}
Where we recognized 
$\eta \cdot \nabla_q - \eta \cdot (d_t\nabla_{\dot q})\mca L
= -\eta \cdot \mbb L = 0$. Substituting back into equation \@ref(eq:separationEquation) 
\[ 
    \mca L(q + \epsilon \eta, \dot q + \epsilon \dot \eta)
    = \mca L(q, \dot q) + \epsilon \mca L(\eta, \dot \eta) + 
    d_t \left[\eta \cdot \nabla_{\dot q}\mca L(q, \dot q)\right]
\] 
Substitute $\epsilon=1$ and integrate to obtain the action. 
The last total-derivative term vanishes by endpoint-preserving 
property of $\eta$. 

<!--chapter:end:02-lagrange-equations.Rmd-->

# Noether's theorem
Noether's theorem elucidates how continuous symmetries 
result in conserved quantities; the converse will be provided 
by Hamiltonian mechanics. 
The key points in this section are: 

1. The definition of a continuous symmetry. 
2. Noether's theorem. 
3. The conserved-quantity when $\mca L$ is time-invariant: the Hamiltonian. 

It's convenient to now define a quantity we'll be using later very often: 

:::{.definition name="canonical momentum"}
Given the Lagrangian $\mca L(q, \dot q, t)$ of a system, 
its canonical momentum is 
\[ 
    p = \nabla_{\dot q} \mca L(q, \dot q, t)
\] 
In terms of the canonical momentum, the Euler-Lagrange 
equations read 
\[ 
    \mbb L_q = \dot p - \nabla_q \mca L = 0
\] 
:::

We will make extensive use of lemma \@ref(lem:calcVariation), which 
we state here again using the canonical momentum for convenience: 

:::{.lemma #calcVariation1 name="ε-derivative of the perturbed Lagrangian"}
Given a path $q:[a, b]\to \R$ and a perturbation path $\eta:[a, b]\to \R$, 
define the parameterized family of paths $q_\epsilon=q + \epsilon \eta$, then 
\[ 
    d_\epsilon \mca L(q_\epsilon, \dot q_\epsilon) 
    = -\eta \cdot \mbb L_q + d_t \left(\eta \cdot p\right)
\] 
:::

:::{.definition name="continuous symmetry"}
Consider a family of maps $q_\epsilon$ such that $q_0$ is the classical 
trajectory w.r.t. the Lagrangian $\mca L$. Define 
\[ 
    \mca L_\epsilon = \mca L(q_\epsilon, \dot q_\epsilon, t)
\] 
as well as the parameterized action function 
\[ 
    S_\epsilon = \int_a^b \mca L(q_\epsilon, \dot q_\epsilon, t)\, dt 
\] 
The physical system has continuous symmetry under the transformation 
parameterized by $q\mapsto q_\epsilon$ if 
\[ 
    d_\epsilon \big|_0 S_\epsilon = C
\] 
where $C$ is a constant independent of the dynamical variables $q, \dot q, t$. 
In other words, infinitesimal reparameterization by $\epsilon$ 
leaves the equations of motion unchanged. 
:::

:::{.proposition name="characterization of continuous symmetry via Lagrangian"}
$\epsilon \mapsto q_\epsilon$ is a continuous symmetry if 
\[ 
    d_\epsilon \big|_0 \mca L_\epsilon = d_t F(q, \dot q, t)
\] 
Note that the right-hand side must be a **total time derivative.**
:::
_Proof:_ Direct computation: 
\[ 
    d_\epsilon \big|_0 S_\epsilon = 
    \int_a^b d_\epsilon \big|_0 \mca L_\epsilon \, dt 
    = \int_a^b d_t(F) \, dt = F(b)-F(a)
\] 

:::{.theorem #noether name="Noether's theorem"}
Given a continuous symmetry $q_\epsilon$ such that 
\[ 
    d_\epsilon \big|_0 \mca L(q_\epsilon, \dot q_\epsilon, t) 
    = d_t F(q, \dot q, t)
\] 
The following Noether change is conserved along a classical 
trajectory obeying the Euler-Lagrange equations 
\[
    Q = \left(d_\epsilon \big|_0 q_\epsilon \right) \cdot 
    \nabla_{\dot q}\mca L(q, \dot q, t) - F(q, \dot q, t)
\]
Let $q_\epsilon = q + \epsilon \eta$ for small $\epsilon$ 
(this is effectively adapting vector field perspective), 
the Noether charge is 
\[ 
    Q = \eta \cdot p - F(q, \dot q, t)
\] 
One special case is for $F$ to vanish, in which case 
\[ 
    d_\epsilon \big|_0 \mca L(q_\epsilon, \dot q_\epsilon, t) = 0
    \implies Q = \eta \cdot p \text{ conserved.}
\] 
:::
_Proof:_ Direct computation invoking lemma \@ref(lem:calcVariation1): 
the $\mbb L_q$ term vanishes on the classical trajectory 
\begin{align}
    d_t F 
    &= d_\epsilon \big|_0 \mca L(q_\epsilon, \dot q_\epsilon) 
    = - \eta \cdot \mbb L_q + d_t(\eta \cdot p) 
    = d_t(\eta \cdot p) 
\end{align}
This implies that the time-derivative of the Noether charge is $0$. 

:::{.example name="the Hamiltonian"}
Let $\mca L$ be independent of time and denote the 
time-translation transform 
\[ 
    q_\epsilon(t) = q(t+\epsilon)
\] 
This is a continuous symmetry with $F(q, \dot q) = \mca L(q, \dot q)$ 
\[ 
    d_\epsilon \big|_0 \mca L(q_\epsilon, \dot q_\epsilon)
    = d_t \mca L
\] 
The associated $\eta = d_\epsilon \big|_0 q_\epsilon 
= d_\epsilon \big|_0 q(t+\epsilon) = \dot q$. Substituting 
into Noether's theorem yields the conserved quantity 
\[ 
    H = \dot q\cdot p - \mca L(q, \dot q)
\] 
:::

<!--chapter:end:03-noethers-theorem.Rmd-->

# Hamiltonian Mechanics 

We begin by exporing the properties of the Legendre 
transform, which relates (convex) Lagrangians and Hamiltonians. 
We next introduce Hamilton's equations, with some 
optional perspective from differential geometry 
which yield some of the most mathematically satisfactory / beautiful 
treatment of the topic. 

Key takeaways of this part are: 

1. The Hamiltonian $H(q, p, t)$ is the Legendre transform of $\mca L(q, \dot q, t)$ 
    fixing $q, t$; fixing $q, t$, the conjugate variables 
    $\dot q$ and $p$ are functions 
    of each other by $\nabla_{\dot q} \mca L = p$ and $\dot q = \nabla_p H$ 
    (lemma \@ref(lem:legendreConjugateRelation)). 
    The Jacobian $J_{p\to q}$ is the Hessian. 
2. When the $\sup$ is dropped in the Legendre transform, 
    the two variables are always understood to obey 
    an implicit equation.
3. On strictly convex functions, the Legendre transform _preserves 
    convexity, and is involutary_. 
4. The scalar Hamiltonian function $H(q, p, t)\in \R$ 
    generates the flow of time in phase space, or time-translation symmetry. 
5. Converse to Noether's theorem: every scalar function on phase space 
    generates a flow corresponding to its continuous symmetry: 

## Legendre transform {-}

The Legendre transform is a general transform of functions 
which interactive particularly well with convex functions. 

:::{.definition name="Legendre transform"}
Given a function $f:\R^n\to \R$, its Legendre transform 
$\mbb L f: \R^n\to \R$ is 
\[ 
    (\mbb L f)(p) = \sup_x \, \la x, p\ra - f(x)
\] 
:::

### Convex functions {-}
Convex functions are characterized by 
\[ 
    f(\lambda x + \bar \lambda y) \leq \lambda f(x) + \bar \lambda f(y), \quad 
    \lambda \in [0, 1], \bar \lambda = 1 - \lambda 
\] 
assume they're well-behaved enough, they're equivalently defined by 
having positive (semi) definite second-derivatives (Hessian) at all points: 
\[ 
    (\mca H_x f)(\forall x) \geq 0, \quad 
    (\mca H_x f)_{ij}(x) = (\pd {x_i x_j}^2 f)(x)
\] 
Here $A\geq 0$ for matrix $A$ is understood 
as _all_ eigenvalues of $A$ being nonnegative. Convexity is further 
equivalent to 
\[ 
    \forall x, y: f(y) \geq \nabla f(x) \cdot (y - x)
\] 
The function $f$ is _strictly convex_ when the inequalities are strict. 

:::{.proposition}
The gradient map $x\mapsto (\nabla f)(x)$ is invertible 
if $f$ is strictly convex. 
:::
_Proof:_ For $x\neq y$, strict convexity implies 
\begin{align}
    \nabla f(x) \cdot (y-x) < f(y) - f(x), \quad 
    \nabla f(y) \cdot (x-y) < f(x) - f(y)
\end{align}
Suppose for contradiction that $\nabla f(x) = \nabla f(y)$, 
substituting $\nabla f(y)=\nabla f(x)$ in the last equation and 
multiplying both sides by $-1$ yields the contradiction 
\[ 
    \nabla f(x)\cdot (y-x) > f(y) - f(x)
\] 

### Properties of the Legendre transform {-}
We now show that the Legendre transform is well-behaved 
for strictly convex functions. 

Assuming $f$ well-behaved, the extremality condition is 
\[ 
    \nabla_x \la x, p\ra - f(x) 
    = p - \nabla_x f(x) = 0 \iff p = \nabla_x f 
\] 
This means that the $\sup$ may be dropped
<span style="color:green">
when it is understood that $x$ is an implict function of 
$p$ via the equation $p = \nabla_x f$ 
</span>: 
\[ 
    (\mbb L f)(p=\nabla_x f) = x\cdot p - f(x)
\] 

The following lemma will prove the Hamilton 
equation $\dot q = \nabla_p H$. 

:::{.lemma #legendreConjugateRelation name="two-way relation between x and p"}
Given a Legendre transform 
\[ 
    g(p) = (\mbb L f)(p) = \sup_x p\cdot x - f(x)
\] 
Then $p = \nabla_x f \iff x = \nabla_p g$. 
:::
_Proof:_ Assuming $p = \nabla_x f$, then we can 
drop the $\sup$ in the Legendre transform, yielding 
\[ 
    g(p) = p\cdot x - f(x)
\] 
Take the gradient w.r.t. $p$ on both sides; note that 
we need to invoke the chain rule since $p, x$ are related: 
\begin{align}
    \nabla_p g(p) 
    &= \nabla_p (p\cdot x - f(x)) 
    = x + J_{p\to x} p - J_{p\to x} (\nabla_x f)_{=p}
    = x 
\end{align}

:::{.proposition name="Jacobian of Legendre transform"}
The Jacobian of the Legendre transform $x\to p$ is the Hessian of $f$ 
\[ 
    J_{x\to p} = d_x p = d_x \left(\nabla_x f\right) = \mca H_x f 
\] 
:::

:::{.proposition name="convexity invariance"}
Suppose $f(x)$ is convex strictly convex so $\mca H_x f> 0$ 
(positive-definite) everywhere, then $\mbb L f$ 
is convex, i.e. $\mca H_p (\mbb L f)>0$. 
:::
_Proof:_ Computing the Hessian explicitly and recognize $\nabla_x f = p$
\begin{align}
    \mca H_p(\mbb L f) 
    &= d_p [\nabla_p (\mbb L f)] = d_p \left[
        x + J_{p\to x} p - J_{p\to x}\nabla_x f
    \right] \\ 
    &= d_p x = J_{p\to x} = J_{x\to p}^{-1} = \left(\mca H_x f\right)^{-1}
\end{align}
Convexity follows from the convexity of $\mca H_x f> 0$. 

:::{.proposition name="involutary"}
For convex $f$, the Legendre transform is involutary 
\[ 
    \mbb L \left[(\mbb L f)(p)\right] (x) = f(x)
\] 
:::
_Proof:_ Unroll the definition
\begin{align}
    \mbb L \left[(\mbb L f)(p)\right] (x)
    &= \mbb L \left[p\mapsto xp - f(x)\right] (x)\\ 
    &= xp - \left[p\mapsto xp - f(x)\right]\, p 
    = xp - xp - f(x) = f(x)
\end{align}
Note the dependence here: fixing $x$, we can fix $p=\nabla_x f$, which 
in turn determines $x$ in the inner Legendre transform. 


## Hamilton's equations {-}

:::{.definition name="Hamiltonian"}
Given the Lagrangian $\mca L(q, \dot q, t)$, the 
    Hamiltonian is the Legendre transform of $\dot q\mapsto p$ 
    holding $q$ fixed 
    \[ 
        H(q, p, t) = p\cdot \dot q - \mca L(q, \dot q(p), t)
    \] 
    where $p = \nabla_{\dot q} \mca L$ and $\dot q(p)$ 
    is the implicit inverse of this equation. 
:::

:::{.theorem #hamiltonEquations name="Hamilton's equations"} 
The following equations are equivalent to the Euler-Lagrange 
    equations \@ref(def:eulerLagrange) when the Lagrangian 
    $\mca L(q, \dot q, t)$ is strictly convex in in $\dot q$: 
    \begin{aligned}
        \dot p 
        = -\nabla_q H, \quad \dot q 
        = \nabla_p H 
    \end{aligned}
    Collecting $\xi = q \oplus p$ i.e. $(\xi_1, \xi_2, \dots, \xi_{2n} = (q_1, p_1, q_2, \dots q_n, p_n)$ 
    and write $H(q, p, t) = H(\xi, t)$, we obtain the compact expression 
    \[ 
        \pd t \xi = \Gamma\,  \nabla_\xi H, \quad \Gamma 
        = \bigoplus \begin{pmatrix}
            0 & 1 \\ -1 & 0 
        \end{pmatrix}
        (\#eq:omegaForm)
    \] 
:::
_Proof:_ Take care that $\dot q, p$ are implicit functions of each other 
    and recognize $p=\nabla_{\dot q}\mca L$; holding $q$ fixed: 
    \begin{align}
        \nabla_p H(q, p, t) 
        &= \nabla_p \left[p\cdot \dot q - \mca L(q, \dot q(p), t)\right] \\ 
        &= \dot q + J_{p\to \dot q} p - J_{p\to \dot q}(\nabla_{\dot q}\mca L) 
        = \dot q 
    \end{align}
    For the second equation, we're holding $p$ fixed, but $\dot q$ is still 
    dependent upon $q$, then 
    \begin{align}
        \nabla_q H(q, p, t) 
        &= \nabla_q \left[
            p\cdot \dot q - \mca L(q, \dot q(p), t)
        \right] \\ 
        &= J_{q\to \dot q}p 
        - \nabla_q \mca L - J_{q\to \dot q} (\nabla_{\dot q}\mca L) 
        = - \nabla_q \mca L  \\ 
        \dot p 
        &= d_t \nabla_{\dot q}\mca L = \nabla_q \mca L 
    \end{align}
    the last equation holds by the Euler-Lagrange equations. 

## Differential geometry perspective {-}

<div style="color:green">
Hamilton's equations $\pd t \xi = \Gamma \, \nabla_\xi H$ 
is the component representation of the following implicit 
equation which defines the vector field $X_H$ 
_generating the time-translation of the system_: 
\[ 
    \omega(X_H, \, \cdot\, ) = dH
\] 
Here $\omega= dq \wedge dp = \sum_j dq_j\wedge dp_j$ 
is the _symplectic form_ represented by $\Gamma$, and $dH$ 
is the differential of $H$. 
</div> 
How is this an implicit equation? Both sides 
of the equation consumes a vector field to output 
a scalar, and $X_H$ is implicitly defined to satisfy the equation. 

We next proceed to showing that this is, in fact, 
equivalent to Hamilton's equations. 

:::{.remark name="differential geometry basics"}
Given a chart $x=(x_1, \cdots, x_n)$ on a manifold $M$ (like fixing 
a basis), a **vector field** $X$ can be understood as a _directional derivative_ 
operation that consumes a scalar function $f$ and outputs $X\, f$ (read as 
$X$ acting on $f$. The value of $(X\, f)(x)$ encodes how much $f$ 
changes along the direction of the vector field $X$ at location $x$. 
The representation of $X$ in this chart $x$ is then 
\[ 
    X = \sum_{j=1}^n X_j \pd j 
\] 
where each $X_j$ is a scalar function and $\pd j = \pd {x_j}$ 
denotes the partial differentiation operation in the $j$-th variable; 
in this representation, the action of $X$ can be locally written 
in coordinate form as 
\[ 
    (X\, f)(x) = \sum_{j=1}^n X_j \pd j f(x) = (X_j)\cdot \nabla f(x)
\] 
Here $(X_j)$ is understood as a vector denoting the "direction"; 
in this sense the chart $x$ also fixes a basis $\pd 1, \cdots \pd n$ 
for the space of vector fields (tangent bundle). 
Given a scalar function $f$, its **differential** $df$ consumes a vector 
field and outputs a scalar (function)
\[
    df\, X = X\, f 
\] 
We can similarly consider a basis representation for the differentials 
(which live on the cotangent bundle) with the notation $d x_1, \cdots, dx_n$ 
\[ 
    df = \sum_{j=1}^N f_j dx_j, \quad dx_j\, \pd i = \delta_{ij}
\] 
:::

The last piece we need is **the wedge product**: assuming a basis 
$B=\{\pd p, \pd q\}$ ($p$, $q$ are univariables), the symplectic form 
$\omega = dq \wedge dp$ consumes two arguments in $B$ and outputs a number 
subject to the following rules, in addition to being linear in all its arguments: 
\[ 
    (dq \wedge dp)(\pd a, \pd b) = \begin{cases}
        1 & (\pd a, \pd b) = \pd q, \pd p \\ 
        -1 & (\pd a, \pd b) = \pd p, \pd q \\ 
        0 & \text{otherwise}. 
    \end{cases}
\] 

Back to Hamilton's equation $\omega(X_H, \cdot) = dH$. 
Fix the basis $\{\pd q, \pd p\}$ (the order matters!)
and consider two vector fields $X, Y$ with components 
\[ 
    X = X_q \pd q + X_p \pd p, \quad Y = Y_q \pd q + Y_p \pd p 
\] 
The representation $\Gamma$ of $\omega$ is read from the equation 
\begin{align}
    \omega(X, Y) 
    &= X_qY_q \omega(\pd q, \pd q)_{=0} + X_p Y_q \omega(\pd p, \pd q)_{=-1}
    + X_qY_p \omega (\pd q, \pd p)_{=1} + X_pY_p \omega (\pd p, \pd p)_{=0} \\ 
    &= X_qY_p - X_p Y_q 
    = \begin{pmatrix} X_q \\ X_p \end{pmatrix}^T 
    \begin{pmatrix}
        0 & 1 \\ -1 & 0 
    \end{pmatrix} \begin{pmatrix}
        Y_q \\ Y_p 
    \end{pmatrix} \implies \Gamma = \begin{pmatrix}
        0 & 1 \\ -1 & 0 
    \end{pmatrix}
\end{align}
The implicit equation holds for all vector fields $Y$ 
(since both sides of the equation consume a vector field and outputs a scalar function)
\[ 
    \omega (X_H, Y) = dH\, Y = Y\, H \implies (X_H)^T \Gamma Y 
    = Y_q \pd q H + Y_p \pd p H = (\nabla H)^T Y, \quad \forall Y
\] 
where we have identified $X_H, Y$ with their $\pd q, \pd p$ coordinate 
representations on the RHS. Solving for $X_H$ yields 
\[ 
    (X_H)^T \Gamma = (\nabla H)^T \iff \nabla H = ((X_H)^T \Gamma)^T 
    = -\Gamma X_H \iff X_H = \Gamma \nabla_H 
\] 
This is exactly the component expression of Hamilton's equations 
we have derived. 
</details>

## Poisson bracket, canonical transformations {-}

From now on, we work with a system with $N$ degrees of freedom 
with phase space of dimension $2N$. Recall the definition 
of $\xi$ and $\Gamma$ in equation \@ref(eq:omegaForm). 

:::{.definition name="Poisson bracket"}
The Poisson bracket between two smooth scalar functions is 
\[
    \pb A B = \nabla_q A \cdot \nabla_p B - \nabla_p A \cdot \nabla_q B 
    = (\nabla_\xi A)^T \Gamma (\nabla_\xi B)
\]
We also use the derivative map $D_A: B\mapsto \pb A B$ to denote partial 
application of the Poisson bracket. 
:::

The quantum analogue of the following relation are the canonical 
commutation relations between $\hat x, \hat p$. They are the true 
foundational axioms which specify a theory. 

:::{.definition name="fundamental Poisson brackets"}
$\pb {q_j} {q_k} = \pb {p_j} {p_k} = 0, \pb {q_j} {p_k} = \delta_{ij}$. 
Equivalently, these quantities are "probing" the entries of $\Gamma$ 
\[
    \pb {\xi_j}{\xi_k} = \Gamma_{jk}
\]
:::

Using the Poisson brackets, Hamilton's equations (theorem \@ref(thm:hamiltonEquations)) 
can be rewritten as 
\[
    \pd t \xi = -D_H \xi \implies \xi(t) = e^{- t D_H} \xi(0).
\]
The vector field $D_H$ is also known as the **symplectic gradient** 
of $H$. The time-evolution operator for time $t$ 
is $\xi \mapsto e^{-tD_H}$. The following result shows the compatibility of the 
Jacobian map with the exponential. 

:::{.proposition name="Jacobian of an exponential transform"}
Given an exponential map $\xi \mapsto e^{-\lambda T}\xi$ 
(here $\xi$ may be an operator instead of a constant matrix!), 
assuming convergence we obtain 
$J_{\xi \mapsto e^{-\lambda T \xi}} = e^{-\lambda J_{\xi \to T\xi}}$. 
:::
_Proof:_ Note that $J_{T+S} = J_T+J_S$, and $J_{TS}=J_TJ_S$. Since 
the exponential is defined as a power series using addition and multiplication, 
we obtain $J_{\exp(T)} = \exp(J_T)$. 

:::{.theorem name="evolution of scalar under flow generated by another scalar"}
Given two scalar functions $C, G$ (observables) 
and $\xi(\lambda)=e^{-\lambda D_G} \xi(0)$, 
the observable $C$ evolves according to 
\[
    d_\lambda C = \pb C G = -D_G C 
\]
To be extremely clear about dependences, we write 
$d_\lambda C(\xi(\lambda)) = -D_G \big|_{\xi(\lambda)} C(\xi(\lambda))$, 
here $C, G$ are both implicit functions of $\lambda$ through $\xi(\lambda)$. 
:::
_Proof:_ Indices which are repeated twice are summed over (contracted), expand to obtain 
\begin{align}
    d_\lambda C 
    &= (\nabla_\xi C)\cdot d_\lambda \xi = -(\nabla_\xi C)\cdot D_G \xi  
    = (\pd {\xi_j} C) [\xi_j, G] \\ 
    &= (\pd {\xi_j} C) \pd {\xi_l} \xi_j \Gamma_{lk} (\pd {\xi_k} G)
    = (\pd {\xi_j} C)\Gamma_{jk}(\pd {\xi_k} G)
    = \pb C G 
\end{align}

:::{.corollary name="scalars are conserved under their Hamiltonian flow"}
Given a scalar function $G$ on phase space, if $\xi$ evolves according to 
the flow generated by $G$ as below, then $d_\lambda G(\xi(\lambda)) = 0$. 
\[
    d_\lambda \xi = - D_G \xi \iff \xi(\lambda) = e^{- \lambda D_G} \xi(0)
\]
:::
_Proof:_ By antisymmetry of the Poisson bracket, $d_\lambda G = \pb G G = 0$. 


:::{.theorem}
:::

## Liouville's theorem {-}

## Optional: probability on phase space {-}

<!--chapter:end:04-Hamiltonian-mechanics.Rmd-->

# Spacetime geometry 

We introduce the Minkowski metric and the Lorentz group, and 
show that the algebra is neatly represented by $2\times 2$ complex matrices. 

Key takeaways: 

1. The Pauli matrices form a basis for 
    $2\times 2$ complex Hermitian matrices (proposition \@ref(prp:pauliProperties)). 
2. Representation of $(3+1)$-spacetime using 
    Hermitian-matrixs (definition \@ref(def:spacetimeRepresentation)). 
3. In this representation, determinant (of the $2\times 2$ Hermitian matrix)
    correspond to the Minkowski norm in $\R^4$ 
    (proposition \@ref(prp:minkowskiMetricDet)). 
4. Conjugation in the representation corresponds to a linear transformation 
    in spacetime (theorem \@ref(thm:conjugateTransform)). 
5. We are interested in the Lorentz group (definition \@ref(def:lorentzGroup))
    which preserve the Minkowski norm. Such (linear) Lorentz transforms 
    are represented the conjugate action of $\mrm{SL}(2, \mbb C)$, or 
    $2\times 2$ complex matrices with determinant 1
    (proposition \@ref(prp:lorentzCondition)). 
    - Rotations are represented by unitary conjugation $SU(2)$ 
        (theorem \@ref(thm:unitariesRotation)). 
    - Lorentz boosts are represented by Hermitian conjugation 
        $\mca H_2\cap \mrm{SL}(2)$ 
        (theorem \@ref(thm:hermitianBoost)). 
7. 

In this section, we work with $2\times 2$ complex matrices. 
Some terminology for reference:

1. $\mrm{GL}(2, \mbb C)$ denotes the set of $2\times 2$ complex invertible 
    matrices. 
2. $\mrm{SL}(2, \mbb C)\subsetneq \mrm{SL}(2, \mbb C)$ denotes the set 
    of $2\times 2$ matrices with $\det A=1$. 
3. $U(2)$ denotes the set of $2\times 2$ unitary matrices 
    satisfying $AA^\dag = A^\dag A = \mbf 1$. 
    The intersection $U(2)\cap \mrm{SL}(2, \mbb C)$ is denoted $SU(2)$, 
    or the special unitary group. 
3. $\mca H_2$ denotes the set of $2\times 2$ Hermitian matrices 
    satisfying $A^\dag = A$, where $A^\dag$ is the conjugate transpose. 

## Matrix representation of spacetime {-}

In this section, we define a representation of spacetime 
via a bijection between $\mbb R^4$ and $\mca H_2$ 
(definition \@ref(def:spacetimeRepresentation)) and 
introduce the Minkowski metric \@ref(def:minkowski). 
the space $\mbb R^4\cong \mbb R\times \mbb R^3$ will be 
understood as $4$-dimensional spacetime. 

Both unitary matrices and Hermitian matrices are _normal_ thus subject to 
the spectral theorem, so one may think of them as a diagonal matrix of eigenvalues 
in some orthonormal basis. In particular: 

1. Hermitian matrices have real eigenvalues. 
2. Unitary matrices have complex eigenvalues with unit norm of the form $e^{i\theta}$. 

An immediate corollary of the observation above is 

:::{.proposition}
Every unitary $U=\exp(iH)$ is the complex exponential of 
some Hermitian matrix $H$. 
:::

Proceeding, recall the Pauli matrices 
\[ 
    \mbf 1 = \begin{bmatrix} 1 \\ & 1\end{bmatrix}, \quad 
    \sigma_x = \begin{bmatrix} & 1 \\ 1 \end{bmatrix}, \quad 
    \sigma_y = \begin{bmatrix} & -i \\ i \end{bmatrix}, \quad 
    \sigma_z = \begin{bmatrix} 1 \\ & -1\end{bmatrix}
\] 
The space $\mca H_2$ of $2\times 2$ Hermitian matrices is a vector space, 
and it comes equipped with the following inner product: 

:::{.definition #hilberSchmidt name="Hilbert-Schmidt inner product"} 
The Hilbert-Schmidt inner product defined on the space of linear operators 
    over a finite-dimensional Hilbert space of dimension $d$ is  
    \[ 
        \la A, B\ra = \df 1 d \mrm{Tr}(A^\dag B)
    \] 
    In a basis representation, this corresponds to the flattened vector inner product. 
:::

The Pauli matrices are special because they form an orthonormal basis for $\mca H_2$: 

:::{.proposition #pauliProperties}
The Pauli matrices are involutary $\sigma_i^2 = \mbf 1$ and 
    form an orthonormal basis for $\mca H_2$ under the Hilbert-Schmidt 
    inner product. 
:::
_Proof:_ Direct computation, one can verify. 

:::{.definition #spacetimeRepresentation name="representation of spacetime"}
We identify points in $\R^4$ 
    \[ 
        x=(x_t, x_1, x_2, x_3)\in \R^4
    \]  
    with $2\times 2$ Hermitian matrices 
    \[ 
        \hat x 
        = x_t\mbf 1 + x_1\sigma_x + x_2\sigma_y + x_3\sigma_3 = 
        \begin{bmatrix} x_t + x_3 & x_1 - i x_2  \\ x_1 + ix_2 & x_t - x_3
        \end{bmatrix}\in \mca H_2
    \] 
    The representation is invertible by Fourier 
    decomposition onto the Pauli basis using \@ref(def:hilberSchmidt): 
    \[ 
        x_t = \la \hat x, \mbf 1\ra = \df 1 2 \mrm{tr}(\hat x\mbf 1), \quad 
        x_{i\in \{1, 2, 3\}} = \la \hat x, \sigma_i\ra = \df 1 2 \mrm{tr}(\hat x\sigma_i) 
    \] 
:::

:::{.definition #minkowski name="Minkowski metric"}
The Minkowski metric over $\R^4$ is the bilinear map 
    $\la \, \cdot\, , \, \cdot\, \ra_M:\R^4\times \R^4\to \R$. 
    \[ 
        \la x, y\ra_M = x_ty_t - x_1y_1 - x_2y_2 - x_3y_3
    \] 
    We denote the Minkowski norm it induces by $\|\, \cdot\|^2_M:\R^4\to \R$, 
    defined by $\|x\|_M^2 = \la x, x\ra_M$. 
:::

One purpose of the 
definition \@ref(def:spacetimeRepresentation) is the following 

:::{.proposition #minkowskiMetricDet}
Under the Pauli identification of $\R^4$
    \[ 
        \det \hat x = \la x, x\ra_M 
    \] 
:::
_Proof:_ Direct computation: $\det \hat x = x_t^2 - x_1^2 - x_2^2 - x_3^2$. 

## The Lorentz group {-}

We wish to investigate linear transformations on $\R^4$ which leave the Minkowski 
metric invariant, since these transformations form the Lorentz group. 

:::{.definition #lorentzGroup name="Lorentz group"}
The Lorentz group $\mca L$ is the subgroup of $\mrm{GL}(4, \R)$ (group of 
    invertible linear transformations on $\R^4$) which preserve the Minkowski metric 
    \[ 
        A\in \mca L\iff \forall x, y\in \R^4, \la x, y\ra_M = \la Ax, Ay\ra
        \iff \forall x\in \R^4, \|x\|^2_M = \|Ax\|^2_M
    \] 
:::

Recall that inverse-conjugating any matrix $A\mapsto BAB^{-1}$ 
will not change the eigenvalues of $A$. In particular, inverse-conjugating 
$\hat x\in \mca H_2$ by any $2\times 2$ invertible matrix yields 
another element of $\mca H_2$; since $\mca H_2$ represent 
points in spacetime, it is natural to ask the 
$H\mapsto BHB^{-1}$ induces on $\R^4$.

:::{.theorem #conjugateTransform name="conjugation-induced linear transform"}
conjugation in $\mca H_2$ yields a linear transformation in $\R^4$: 
    Equivalently, 
    the map $A:\R^4\to \R^4$ which makes the diagram 
    below commute is a linear operator. 

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(rep("images/LorentzRepresentation.png", 1))
```
In particular, the representation of $\hat A$ as a matrix in $\R^4$ is
    \[ 
        A_{ij} = (Ae_j)_i = \la \hat A \sigma_j \hat A^\dag, \sigma_i \ra 
        = \df 1 2 \mrm{tr}\left(\hat A \sigma_j \hat A^\dag\sigma_i\right) 
        (\#eq:conjugationRepresentation)
    \] 
:::
_Proof_: Fixing $\hat A:\mca C^2\to \mca C^2$ and let $x, y\in \R^4$.
    Let $\sigma_i$ be any Pauli basis matrix, we show that $A$ is linear: 
\begin{align}
    \left[A(x+\alpha y)\right]_i &= 
    \la \hat A(\hat x + \alpha \hat y)\hat A^\dag, \sigma_i\ra 
    = \df 1 2 \mrm{tr}\left(\hat A (\hat x + \alpha \hat y)\hat A^\dag \sigma_i \right) \\ 
    &= \df 1 2 \mrm{tr}\left(\hat A \hat x \hat A^\dag \sigma_i \right)  + 
        \df 1 2 \alpha \mrm{tr}\left(\hat A \hat x \hat A^\dag \sigma_i \right) 
    = (Ax)_i + \alpha(Ay)_i 
\end{align}

We are interested in the Lorentz group, i.e. induced transformations $A:\R^4\to \R^4$ which 
preserve the Minkowski metric. 
The relation between Minkowski norm and determinant of the Pauli identification in
proposition \@ref(prp:minkowskiMetricDet) gives a convenient characterization. 

:::{.proposition #lorentzCondition}
Conjugation by $\hat A:\mbb C^2\to \mbb C^2$ induce a Lorentz transformation only if 
    \[ 
        \det \hat A = \pm 1
    \] 
    Moreover, $\hat A$ induces the same transformation as $-\hat A$, 
        so without of loss of generality we can take $\det \hat A = 1$. 
:::
_Proof_: Use proposition \@ref(prp:minkowskiMetricDet): 
$\|Ax\|^2_M = \det(\hat A\hat x\hat A^\dag) 
    = \det \hat x (\det A)^2 = (\det A)^2 \|x\|^2_M$

Within $\mrm{SL}(2, \mbb C)$, the special unitary group $\mrm{SU}(2)$ consisting of $2\times 2$ 
unitaries and the unit-determinant Hermitians $\mca H_2\cap \mrm{SL}(2, \mbb C)$ are special: 
every $A\in \mrm{SL}(2, \mbb C)$ may be decomposed into a unique product of 
$U\in \mrm{SU}(2)$ and $H\in \mca H_2\cap \mrm{SL}(2, \mbb C)$. 

## Boosts and rotations {-}

We explore $\R^4$ transforms induced by the unitaries and unit-determinant Hermitians, 
beginning with the unitaries. Recall that every unitary $U=\exp(iH)$ for some Hermitian $H$. 
Since the Paulis form a basis, every unitary $U$ may be written uniquely as 
\[ 
    U=\exp\left[i(x_t \mbf 1 + x_1\sigma_x + x_2\sigma_y + x_3\sigma_z)\right] = 
    \exp\left[ix_t + i(x_1\sigma_x + x_2\sigma_y + x_3\sigma_z)\right]
\] 
Inspecting equation \@ref(eq:conjugationRepresentation) shows that the induced transformation is 
invariant in $x_t$, and, in fact, an overall scaling of $(x_1, x_2, x_3)$. Without loss of 
generality we consider unitaries of the form $U=\exp(i(\hat r\cdot \sigma))$, for $r\in S^2$,
the unit $2$-sphere in $\R^3$. 

:::{.theorem #unitariesRotation name="unitaries induce spatial rotations"}
 A clockwise rotation $A$ in the last three spatial coordinates about $\hat r\in S^2$ by $\theta$ 
    is induced via equation \@ref(eq:conjugationRepresentation) by 
    \[ 
        U(\hat r, \theta) = 
        \exp\left[-\df i 2 \theta (\hat r\cdot \vec \sigma)\right] \in \mrm{SU}(2)
    \] 

_Proof:_ All conjugation fixes the time axis since $\mbf 1$ commutes 
    with everything so $U(x_t\mbf 1)U^\dag = x_t\mbf 1$. 
    The only Minkowski-norm preserving isometries of $\R^4$ which fix time
    are the spatial rotations. The direct form follows by direct computation or 
    noting the identification with quaterions. We do one example here, let 
    \[ 
        \hat r = (0, 0, 1), \quad \hat R_z(\theta) = \exp\left(-\df i 2 \theta\sigma_z\right)
        \implies R_z(\theta)_{ij} = \df 1 2 \mrm{tr}\left(\hat R_z(\theta)\sigma_j 
        \hat R_z(-\theta)\sigma_i \right)
    \] 
    If $j=3$ (corresponding to spatial $z$) or $j=0$ (time), 
    $\sigma_j$ commutes with $\hat R_z(\theta)$ so 
    \[ 
        R_z(\theta)_{ij\notin\{0, 3\}}=\delta_{ij}
    \] 
    This implies that $R_z(\theta)$ acts trivially on the time and $z$-axes. 
    Next, let us consider the subspaces spanned by $\sigma_1, \sigma_2$. Recall that 
    $[\sigma_3, \sigma_1] = -[\sigma_1, \sigma_3]$ 
    \begin{align}
        R_z(\theta)_{11} 
        &= \df 1 2 \mrm{tr}\left(e^{-i\theta \sigma_3/2}\sigma_1 e^{i\theta \sigma_3/2}\sigma_1 \right) \\ 
        &= \df 1 2 \mrm{tr}\left(e^{-i\theta \sigma_3}\sigma_1\sigma_1 \right) 
        = \df 1 2 \mrm{tr}\left(e^{-i\theta \sigma_3}\right) \\ 
        &= \df {e^{i\theta} + e^{-i\theta}} 2 = \cos\theta 
    \end{align}
    The same applies to $R_z(\theta)_{22}$
    Continuing the calculation and remembering the commutation relation 
    $\sigma_i\sigma_j = i\epsilon_{ijk}\sigma_k$ 
    \begin{align}
        R_z(\theta)_{12} 
        &= \df 1 2 \mrm{tr}\left(e^{-i\theta \sigma_3/2}\sigma_1 e^{i\theta \sigma_3/2}\sigma_2 \right) \\ 
        &= \df 1 2 \mrm{tr}\left(e^{-i\theta \sigma_3}\sigma_1\sigma_2 \right) 
        = \df 1 2 \mrm{tr}\left(e^{-i\theta \sigma_3}i\sigma_3\right) \\ 
        &= \df {ie^{i\theta} - ie^{-i\theta}} 2 = \df{e^{-i\theta}-e^{i\theta}}{2i} = -\sin\theta
    \end{align}
    Then $R_z(\theta)$ restricted to the $xy$ plane is a spatial rotation: 
    \[ 
        R_z(\theta)\big|_{xy} = 
        \begin{bmatrix}
            \cos\theta & -\sin\theta \\ 
            \sin\theta & \cos\theta 
        \end{bmatrix}
    \] 
:::

We spend some time here to recall the hyperbolic functions. 
\begin{align}
    \cosh x &= \sinh'x = \df {e^x + e^{-x}} 2 \\ 
    \sinh x &= \cosh'x = \df {e^x - e^{-x}} 2 \\ 
    \sinh(x+y) &= \sinh x \cosh y + \cosh x \sinh y \\ 
    \cosh(x+y) &= \cosh x \cosh y + \sinh x \sinh y \\ 
    1 &= \cosh^2 x - \sinh^2 x 
\end{align}

:::{.theorem #hermitianBoost name="special Hermitians induce Lorentz boosts"}
Given $\hat r\in S^2, \chi\in \R$, the following Hermitian induces a Lorentz boost 
    in the direction of $\hat r\in S^2$ by $\chi$. 
    \[ 
        H(\hat r, \chi) = \exp \left[-\df 1 2 \chi (\hat r\cdot \sigma)\right]\in \mca H_2 
    \] 
:::
_Proof:_ explicit calculation. 
Given $H(\hat r, \chi)$, compute the matrix elements of the linear transform of 
$\R^4$ it corresponds to using 
equation \@ref(eq:conjugationRepresentation); this turns out to be the 
Lorentz boost. 

<!--chapter:end:05-spacetime-geometry.Rmd-->

# Bibliography {-}

<!--chapter:end:99-Bibliography.Rmd-->

