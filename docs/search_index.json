[["index.html", "Physics 151 F24 Notes Preface Introduction to the course Using these notes A note on notation", " Physics 151 F24 Notes Nicholas Lyu, Arthur Jaffe 2024-10-15 Preface Introduction to the course For math concentrators, at some point (late into high school or in college) there is a change of perspective from computation to proof-based math. For example, one stops viewing matrices as large blobs of numbers but as coordinate representations of abstract linear transformations. There is a similar shift in perspective in physics, when the emphasis changes from solving equations of motion to understanding the fundamental reasons they’re there in the first place. Key to navigating this change is understanding the role of symmetry and conservation laws, as well as the importance of operators in physics. The focus of this course is not quantum theory (maybe at the end, time permitting), but the classical treatment of symmetry and conservation will help motivate much of quantum theory’s constructions. Using these notes These notes accompany the Fall 2024 iteration of Arthur Jaffe’s Mechanics course at Harvard (Physics 151). They recount main results and, occasionally, supplement Prof. Jaffe’s lecture notes on canvas. It can serve as a concise reminder of the results in lecture. The main deliverables are: Covariant and extremal-action perspectives on Lagrangian mechanics. Connection between Lagrangian and Hamiltonian mechanics via the Legendre transform. Nother’s theorem: continuous symmetry \\(\\implies\\) conserved quantity. Spacetime geometry: unitary representation of (a component of) the Lorentz group. A note on notation It is always important to be careful about derivative maps, where overloaded notation can frequently lead to confusion. We use the following standard notation for partial derivatives: \\[ \\partial_{x_1} f(x_1, x_2) = \\dfrac{\\partial}{\\partial x_1} f(x_1, x_2) \\] For second-order derivatives, we adopt \\[ \\partial_{x_1, x_2}^2 f(x_1, x_2) = \\dfrac{\\partial^2}{\\partial x_1\\partial x_2} f(x_1, x_2) \\] We also use the following not-so-standard notation for total derivatives: \\[ d_t f(x_1(t), x_2(t)) = \\dfrac d {dt} f(x_1(t), x_2(t)) \\] When we wish to specify the point at which a derivative is evaluated, we write e.g.  \\[ d_t\\big|_0 f(x_1(t), x_2(t)) = x&#39;_1(0) \\partial_{x_1} f + x&#39;_2(0) \\partial_{x_2} f \\] "],["two-body-problem.html", "1 Two-body problem Problem setup Three conserved quantities Orbits, effective potential Scattering", " 1 Two-body problem We approach the two-body problem in the following steps: Reduce to one-body motion. Identify the conserved quantities: energy \\(E\\), angular-momentum \\(\\mathbf L\\), and Runge-Lenz vector \\(\\boldsymbol\\epsilon\\). \\(E\\) is conserved when the potential \\(V\\) is time-invariant. \\(\\mathbf L\\) is conserved when \\(V\\) is central. \\(\\boldsymbol\\epsilon\\) is conserved when \\(V\\propto r^{-2}\\). Orbits of such potentials have conserved eccentricity. Reduce to planar motion using conservation of \\(\\mathbf L\\). Derive the orbit equations from conserved quantities. Analyze the different kinds of orbits by looking at the one-dimensional effective potential \\(V_{\\mathrm{eff}}\\). Analyze scattering. Problem setup Definition 1.1 (two-body problem) Consider two particles with mass \\(m_1, m_2\\) at locations \\(\\mathbf x_1, \\mathbf x_2 \\in \\mathbb R^3\\). Their potential is \\[ V(\\mathbf x_1, \\mathbf x_2) = -\\dfrac{k}{|\\mathbf x_1-\\mathbf x_2|} \\] Here \\(k\\in \\mathbb R\\) is a constant. The interaction is attractive when \\(k&gt;0\\) and repulsive when \\(k&lt;0\\). Computing the force based on the potential: \\[ \\mathbf F_j = -\\nabla_{\\mathbf x_j}V(\\mathbf x_1, \\mathbf x_2) = \\dfrac{k}{|\\mathbf x_1 - \\mathbf x_2|^2} \\nabla_{\\mathbf x_j}|\\mathbf x_1 - \\mathbf x_2| = \\dfrac{k}{|\\mathbf x_1-\\mathbf x_2|^2} \\begin{cases} (\\mathbf x_1 - \\mathbf x_2) &amp; j = 1 \\\\ (\\mathbf x_2 - \\mathbf x_1) &amp; j = 2 \\end{cases} \\tag{1.1} \\] Note that \\(\\mathbf F_1 + \\mathbf F_2 = 0\\). Recalling Newton’s second law, the equations of motion are \\[ \\ddot {\\mathbf x}_j = \\mathbf F_j / m_j \\] Instead of solving for \\(\\mathbf x_1, \\mathbf x_2\\), one can solve instead for the motion of the center of mass \\(\\mathbf R\\) and displacement \\(\\mathbf r\\). The motion of \\(\\mathbf R\\) will be trivial. Proposition 1.1 (center of mass motion) Define the center of mass \\[ \\mathbf R = \\dfrac{m_1}{m_1+m_2} \\mathbf x_1 + \\dfrac{m_2}{m_1+m_2}\\mathbf x_2 \\] The equation for the center of mass is \\[ (m_1+m_2)\\ddot {\\mathbf R} = m_1\\ddot{\\mathbf r}_1 + m_2\\ddot{\\mathbf r}_2 = \\mathbf F_1 + \\mathbf F_2 = 0 \\] The motion of the center of mass is thus completely determined by the initial conditions of the problem. Definition 1.2 (relative coordinates, displacement) Define the relative coordinates \\(\\mathbf r_j = \\mathbf x_j - \\mathbf R\\) and displacement \\(\\mathbf r = \\mathbf x_2 - \\mathbf x_1\\). Denote by \\(r=|\\mathbf r|, \\mathbf n = \\mathbf r / r\\). Note that \\(\\mathbf n\\) points in the direction \\(1\\to 2\\). Proposition 1.2 (one-body equation) The coordinates \\(\\mathbf r\\) obeys \\[ \\ddot {\\mathbf r} = \\mathbf F/\\mu, \\quad \\mathbf F = -\\nabla_{\\mathbf r} V(r) = -\\dfrac{k}{r^3} \\mathbf r \\quad \\mu = \\dfrac{m_1m_2}{m_1+m_2} \\] Here \\(\\mu\\) is the reduced mass which satisfies \\(1/m_1 + 1/m_2 = 1/\\mu\\). It is the “effective” mass of the one-body “particle” corresponding to the two-body problem. Additionally define the one-body momentum \\[ \\mathbf p = \\mu \\dot {\\mathbf r}, \\quad \\dot {\\mathbf p} = \\mathbf F \\] Proof: Rewriting (1.1) in terms of the newly defined quantities: \\[\\begin{align} m_1 \\ddot {\\mathbf x}_1 &amp;= \\dfrac{k}{r^2}\\mathbf n = -m_2 \\ddot {\\mathbf x}_2 \\\\ \\ddot {\\mathbf r} &amp;= \\ddot {\\mathbf x}_2 - \\ddot {\\mathbf x}_1 = -\\left(\\dfrac 1 {m_2} + \\dfrac 1 {m_1}\\right)\\dfrac{k}{r^2} \\mathbf n = - \\dfrac{k}{\\mu r^2}\\mathbf n \\end{align}\\] Three conserved quantities Definition 1.3 (conserved quantity (non-relativistic)) In Newtonian mechanics, a quantity is ‘’conserved’’ if it remains constant under time-evolution. Proposition 1.3 (conservation of energy) The energy scalar \\[ E = T + V = \\dfrac{\\mathbf p^2}{2\\mu} + V \\] is conserved by Newton’s equation of motion \\(\\mu \\ddot {\\mathbf r} = \\mathbf F = -\\nabla_{\\mathbf r} V(r)\\) Proof: Direct computation \\[ d_t E = d_t \\left( \\dfrac{\\mu^2 \\dot {\\mathbf r} \\cdot \\dot {\\mathbf r}}{2\\mu} + V \\right) = \\mu \\dot {\\mathbf r}\\cdot \\ddot {\\mathbf r} - (\\nabla_{\\mathbf r} V) \\cdot \\mathbf r = 0 \\] Note that \\(d_tV\\) is computed according to the dependence \\(V\\leftarrow \\mathbf r\\leftarrow t\\). Proposition 1.4 (conservation of angular momentum) The angular momentum vector \\[ \\mathbf L = \\mathbf r\\times \\mathbf p \\] Is conserved for any central force problem: one in which \\(V(\\mathbf r)=V(r, \\mathbf n) = V(r)\\) is only dependent on the magnitude, but not direction, of \\(r\\). Proof: Direct computation \\[ d_t \\mathbf L = d_t(\\mathbf r\\times \\mathbf p) = \\dot {\\mathbf r}\\times \\mathbf p + \\mathbf r \\times \\mathbf F = 0 + 0 \\] The first term vanishes by \\(\\mathbf p \\parallel \\dot {\\mathbf r}\\) and the second by the definition of central potential. Remark. This is our first example of symmetry-conservation. A central force problem demonstrates spherical symmetry. A rigorous definition of “symmetry” will be given soon. Proposition 1.5 (planar reduction) The trajectory \\(\\mathbf r\\) lie in the plane orthogonal to \\(\\mathbf L\\). Given this, let \\(\\mathbf n =\\mathbf r/r\\) be the first unit component of the plane and the second \\[ \\mathbf l = (\\mathbf L \\times \\mathbf r) / |\\mathbf L\\times \\mathbf r| \\] In polar coordinates, \\[ \\dot {\\mathbf r} = \\dot r \\mathbf n + r\\dot \\theta \\mathbf l \\implies \\mathbf L = \\mu r^2 \\dot \\theta (\\mathbf n\\times \\mathbf l) \\] Proof: \\(\\mathbf L = \\mu \\, \\mathbf r \\times \\dot {\\mathbf r}\\) is orthogonal to both \\(\\mathbf r\\) and \\(\\dot {\\mathbf r}\\), and \\(\\mathbf L\\) is conserved. Proposition 1.6 (conservation of the Runge-Lenz vector) The Runge-Lenz vector \\(\\boldsymbol\\epsilon\\) is defined as \\[ \\boldsymbol\\epsilon = \\dfrac 1 {\\mu k} \\mathbf p \\times \\mathbf L - \\mathbf n = \\dfrac 1 {\\mu k} [\\mathbf p \\times (\\mathbf r\\times \\mathbf p)] - \\mathbf n = \\dfrac 1 {\\mu k} [\\mathbf p \\times (\\mathbf r\\times \\mathbf p)] - \\mathbf n = \\dfrac{pL} {\\mu k} \\mathbf n - \\mathbf n \\] The vector is conserved. Proof: Direct computation. \\[\\begin{align} \\dot {\\boldsymbol\\epsilon} &amp;= \\dfrac 1 {\\mu k} \\mathbf F \\times \\mathbf L - \\dot {\\mathbf n} = \\dfrac 1 {\\mu k} \\left(-\\dfrac{k}{r^2} \\mathbf n\\right) \\times [\\mu r^2 \\dot \\theta (\\mathbf n \\times \\mathbf l)] - \\dot {\\mathbf n} \\\\ &amp;= -\\dot \\theta [\\mathbf n\\times (\\mathbf n \\times \\mathbf l)] - \\dot {\\mathbf n} = \\dot \\theta \\mathbf l - \\dot {\\mathbf n} = 0 \\end{align}\\] Proposition 1.7 (magnitude of the Runge-Lenz vector) The magnitude \\(\\epsilon = |\\boldsymbol\\epsilon|\\) is \\[ \\epsilon^2 = 1 + \\dfrac{2EL^2}{\\mu k^2} \\] Proof: First compute \\(\\mathbf p\\times \\mathbf L = \\mathbf p \\times (\\mathbf r \\times \\mathbf p) = Lp\\mathbf n\\). Then \\(\\mathbf n \\cdot (\\mathbf p\\times \\mathbf L) = Lp\\) and \\(|\\mathbf p\\times \\mathbf L|^2 = p^2L^2\\) since \\(\\mathbf p \\perp \\mathbf L\\). Also note that \\(p/L = 1/r\\), then then \\[\\begin{align} \\epsilon^2 &amp;= \\mathbf n\\cdot \\mathbf n + \\dfrac{1}{\\mu^2 k^2} [(\\mathbf p\\times \\mathbf L)\\cdot (\\mathbf p\\times \\mathbf L)] - \\dfrac{2}{\\mu k} \\mathbf n \\cdot (\\mathbf p\\times \\mathbf L) \\\\ &amp;= 1 + \\dfrac{p^2 L^2}{\\mu^2k^2} - 2 \\dfrac{pL}{\\mu k} = 1 + \\dfrac{2L^2}{\\mu k^2}\\left(\\dfrac{p^2}{2\\mu} - \\dfrac{pk}{L}\\right) = 1 + \\dfrac{2EL^2}{\\mu k^2} \\end{align}\\] Orbits, effective potential We can obtain an implicit equation of motion using the conservation equations. By definition of \\(\\boldsymbol\\epsilon\\) 1.6 \\[ \\mathbf r\\cdot \\boldsymbol\\epsilon = \\dfrac{rpL}{\\mu k} - r = \\dfrac{L^2}{\\mu k} - r \\] Let \\(\\mathbf r\\cdot \\boldsymbol\\epsilon = r\\epsilon \\cos\\theta\\), then the orbit equation reads \\[ r(1+\\cos\\theta) = \\dfrac{L^2}{\\mu k} \\tag{1.2} \\] Remark. Here the orientation is such that \\(\\theta=0\\) points along \\(\\boldsymbol\\epsilon\\). This is also an implicit orbit equation since time is not an explicit variable. The lecture note appendix works out the conic section trajectories corresponding to different ranges of \\(\\epsilon\\). Definition 1.4 (effective radial potential) So far, the kinetic energy is computed according to the vector derivative \\(\\dot {\\mathbf r}\\) \\[ E = T + V = \\dfrac 1 2 \\mu \\dot {\\mathbf r}^2 - \\dfrac{k}{r} \\] In our case, we can reduce this to a scalar problem by introducing the effective potential. Recall \\(\\dot {\\mathbf r} = \\dot (r\\mathbf n) = \\dot r\\mathbf n + r\\dot {\\mathbf n} = \\dot r\\mathbf n + r\\dot \\theta \\mathbf l \\implies \\dot {\\mathbf r}^2 = \\dot r^2 + r^2\\dot \\theta^2 = \\dot r^2 + L^2/\\mu^2 r^2\\) \\[ E = \\dfrac 1 2 \\mu \\dot{\\mathbf r}^2 - \\dfrac k r = \\dfrac 1 2 \\mu \\dot r^2 + \\left(\\dfrac{L^2}{2\\mu r^2} - \\dfrac k r \\right) \\] The last term in paranthesis is called the “effective potential \\(V_{\\mathrm{eff}}\\). Remark. Recall that \\(L\\) is conserved when the potential displays spherical symmetry. The effective potential conveniently reduces a vector problem into a scalar problem at the cost of introducing a “centrifugal” term. This is a prime example of symmetry helping simplify analysis. The effective potential reduction will show up again in the QM treatment of the hydrogen atom (Physics 143a) and spherical gravity correction (Physics 210). Energy is conserved along orbits, and we can easily identify the bound, unbound, and spherical orbits from the effective-potential graph. Exercise 1.1 (relativistic gravitational effective-potential) Identify the effective potential for Newtonian gravity, where \\(k=GM\\). What is the radius of the circular orbit? The effective radial potential from a spherical mass according to general relativity is \\[ V_{\\mathrm{eff}} = -\\sigma \\dfrac{GM}{r} + \\dfrac{L^2}{2r^2} - \\dfrac{GML^2}{r^3} \\] Where \\(\\sigma=0\\) if the particle is massless else \\(1\\). What are the behaviors for massive and massless particles? Scattering In scattering problems, we assume a spatially and temporally uniform distribution of incoming beams of particles along the incident \\(z\\)-axis. We use cylindrical coordinates \\((z, b, \\phi)\\) denoting height, radius, and the azimuthal angle, respectively. The main quantities are: Impact parameter \\(b\\), scattering angle \\(\\theta\\). Particles are incident within an infinitesimal patch with cross-sectional area \\(d\\sigma(b, \\phi) = b\\, db\\, d\\phi\\). Particle emission are considered according to a solid angle \\(d\\Omega(\\theta, \\phi) = \\sin\\theta \\, d\\theta\\, d\\varphi\\). The differential cross-section \\(d_\\Omega\\sigma \\equiv \\dfrac{b}{\\sin\\theta}|d_\\theta b|\\). Usually, the greater \\(\\theta\\) (heavier scattering effect), the smaller \\(b\\), since incident particles shoot closer to the scattering source – thus the absolute sign. The total cross-section \\(\\sigma = \\int (d_\\Omega \\sigma)\\, d\\Omega\\). This is the total cross-sectional area which encounters scattering. For classical hard-sphere scattering, this is \\(\\pi R^2\\). Remark. When the scattering source is spherically symmetric, all dependences on \\(\\phi\\) can be dropped. The differential cross-section \\(d_\\Omega \\sigma\\) asks: at angle \\(\\theta\\) from scattering center, what is the impact parameter \\(b\\) and how much unit increase in output solid angle will be able to account for unit area increase in the incident beams? To compute the two-body scattering differential cross-section, the first step is deriving the relation between the impact parameter \\(b\\) and scattering angle \\(\\theta\\). Proposition 1.8 (scattering relation) \\(b(\\theta) = \\dfrac{|k|}{2E} \\cot(\\theta/2)\\) Proof: Recall the orbit equation (1.2): denote the planar angle \\(\\phi\\), let \\(D = \\dfrac{L^2}{\\mu k}\\), and expand in terms of \\(x, y\\) \\[ r(1 + \\epsilon \\cos\\phi) = r + \\epsilon x = D \\] Expanding in terms of \\(x, y\\) \\[\\begin{align} r^2 &amp;= x^2 + y^2 = (D - \\epsilon x)^2 \\\\ y^2 &amp;= (\\epsilon^2 - 1)x^2 - 2D\\epsilon x + D^2 \\end{align}\\] For repulsive orbits, \\(\\epsilon&gt;1\\) and the orbits asymptote to \\[ y \\sim \\pm \\sqrt{\\epsilon^2 - 1} x, \\quad |x|\\to \\infty \\] The scattering angle \\(\\theta\\) in the scattering diagram thus satisfy \\[\\begin{align} \\tan\\theta_{\\mathrm{max}} = \\sqrt{\\epsilon^2 - 1}, \\quad \\tan(\\theta/2) = \\tan(\\pi/2 - \\theta_{\\mathrm{max}}) = \\cot(\\theta_{\\mathrm{max}}) \\end{align}\\] The angular momentum \\(L\\) of the incoming particle can be computed at the incident limit: \\[\\begin{align} L &amp;= |\\mathbf r\\times \\mathbf p| = b p = b\\sqrt{2\\mu E} \\\\ \\cot(\\theta/2) &amp;= \\tan(\\theta_{\\mathrm{max}}) = \\sqrt{\\dfrac{2EL^2}{\\mu k^2}} = \\dfrac{2bE}{|k|} \\end{align}\\] Theorem 1.1 (Rutherford cross-section formula) For repulsive two-body scattering, the cross-section is dependent upon \\(\\theta\\) as \\[ (d_\\Omega \\sigma)(\\theta) = \\left(\\dfrac{k}{4E}\\right)^2 \\sin^{-4}\\dfrac{\\theta}{2} \\] Proof: The first step is elucidating the dependence \\(d_\\Omega \\sigma \\leftarrow b,\\, d_\\theta b \\leftarrow \\theta\\). From 1.8 we have \\[\\begin{align} b(\\theta) &amp;= \\dfrac{|k|}{2E}\\cot(\\theta/2) \\\\ d_\\theta b &amp;= -\\dfrac{|k|}{4E} \\mathrm{csc}^2(\\theta/2) \\end{align}\\] Substituting into the relation \\(d_\\Omega \\sigma = b|d_\\theta b|/\\sin\\theta\\) yields formula as claimed. "],["lagranges-equations.html", "2 Lagrange’s Equations Covariant formulation Extremization formulation Representation perspective Separability of quadratic Lagrangians", " 2 Lagrange’s Equations In this section we develop two perspectives on Lagrange’s equations: A “covariant” formulation of Newton Cartesian equations. Equation of the trajectory satisfying the principle of least action. We note in the subfinal section that the two perspectives are related: the Euler-Lagrange vector \\(\\mathbb L_q\\) is (negative) the representing element of the directional derivative map \\(\\eta \\mapsto (D_\\eta \\mathcal S)(q)\\), which is a linear functional in the endpoint-preserving perturbation \\(\\eta\\). Both perspectives are extremely important to the development of modern theoretical physics. Computationally, make sure to understand lemma 2.5, which appears again in the proof of Noether’s theorem. Covariant formulation Covariance: introduction What do we mean by something covariant? Consider a function \\(f:\\mathbb R^2\\to \\mathbb R\\) defined on the 2D plane according to \\[ f_1(x, y) = x^2+y^2 \\] This function is, in a sense, “equivalent” to the definition \\[ f_2(r, \\theta) = r^2 \\] Although \\(f_1, f_2\\) are defined on different domains, they represent the same function \\(f\\) defined on the \\(2D\\) plane: the choice of \\((x, y)\\) versus \\((r, \\theta)\\) are simply different choices of “charts” to represent the same underlying geometric object (manifold). When we specify \\(f(r, \\theta)=r^2\\) and understand \\((r, \\theta)\\) as a chart for \\(\\mathbb R^2\\), we have equivalently defined \\(f(x, y)=x^2+y^2\\). In this sense, \\(f\\) is covariant. Remark. Covariant objects are, in a sense, analogous to “platonic ideas.” Another prime example of a covariant object is an abstract linear transformation (e.g. rotation): in this case the matrix representations are obtained after fixing a chart (basis). Remark. If you want to dig deeper, study differential geometry! Euler-Lagrange Equations Let \\((q_j)=\\{q_1, \\cdots, q_N\\}\\) denote the degrees of freedom. A trajectory’s “snapshot” at a single time is completely captured by \\(((q_j), (\\dot q_j))\\). A Lagrangian is a scalar (function) which looks at such snapshots and evaluates to a number. Definition 2.1 (Lagrangian) The Lagrangian of a physical system is a map of type \\[ \\mathcal L: \\mathbb R^n\\times \\mathbb R^n \\times \\mathbb R\\to \\mathbb R \\] Its arguments are abbreviated \\(\\mathcal L(q, \\dot q, t)\\) but expands to \\[ \\mathcal L(q_1, \\cdots, q_n, \\dot q_1, \\cdots, \\dot q_n, t) \\] To be precise, the Lagrangian is a function on the tangent bundle \\(TM\\) of the configuration manifold \\(TM\\). Definition 2.2 (Euler-Lagrange Equations) According to the EL-equations, a trajectory \\(q(t)\\) is physical if \\[ 0 = \\mathbb L_q(t) = (d_t \\nabla_{\\dot q} - \\nabla_q)\\mathcal L(q, \\dot q, t) \\in \\mathbb R^n \\] for all \\(t\\). The notation above unpacks to \\(\\forall j=1, \\cdots, n\\). \\[ d_t (\\partial_{\\dot q_j} \\mathcal L) = \\partial_{q_j} \\mathcal L \\] the time-dependent Euler-Lagrange vector \\(\\mathbb L_q\\) captures the deviation from \\(0\\); we use the subscript to denote explicit dependence on the coordinates \\(q\\). In Cartesian coordinates, define the Newtonian Lagrangian to be \\[ \\mathcal L(x, \\dot x, t) = T - V = \\dfrac m 2 \\dot q^2 - V(q, t) \\] Then Lagrange’s equations (definition 2.2) simplify exactly to Newton’s equations. \\[ m\\ddot x = \\nabla_x V \\tag{2.1} \\] Covariance of Euler-Lagrange Equations The following theorem establishes that the following statements are equivalent: Newton’s equations (2.1) are true in Cartesian coordinates Lagrange’s equations are true in arbitrary invertible coordinates. Theorem 2.1 (Covariance of Euler-Lagrange equations) Given a coordinate transform \\(x\\to q\\), the EL-vector transforms covariantly according to \\[ \\mathbb L_q(t) = J_{q\\to x} \\mathbb L_x(t) \\] To prove this, we need to establish several lemmas. We begin by looking at how the change of coordinates \\(q\\to x\\) determines \\((q, \\dot q)\\to (x, \\dot x)\\). Lemma 2.1 (dot cancellation) \\(J_{\\dot q\\to \\dot x} = J_{q\\to x}\\). Proof: Follows from chain rule: \\[\\begin{align} \\dot x_j &amp;= d_t x_j(q_1, \\cdots, q_j) = (\\partial_{q_k} x_j) \\dot q_k \\implies \\dot x = J_{q\\to x} \\dot q \\end{align}\\] Now, \\(J_{q\\to x}\\) is independent of \\(\\dot q\\), so \\(J_{\\dot q\\to \\dot x} = J_{q\\to x}\\). Lemma 2.2 \\(J_{q\\to \\dot x} = d_t J_{q\\to x}\\) Proof: On the LHS, \\(\\partial_{q_i} \\dot x_j = \\partial_{q_i} (\\dot q_k \\partial_{q_k} x_j) = \\dot q_k (\\partial_{q_iq_k}^2 x_j)\\). On the RHS, \\((d_t J_{q\\to x}(q_1, \\cdots, q_n))_{ij} = (\\partial_{q_i q_k}^2 x_j) \\dot q_k\\). The next two lemmas essentially restate the chain rule for \\(q(x)\\) and \\(\\dot q(x, \\dot x)\\). Lemma 2.3 \\(\\nabla_{\\dot x} = J_{\\dot q\\to \\dot x} \\nabla_{\\dot q}\\). Lemma 2.4 \\(\\nabla_x = J_{\\dot q\\to x} \\nabla_q + J_{q\\to x}\\nabla_x\\). We are now ready to prove theorem 2.1 by expanding the operator \\(d_t\\nabla_{\\dot x} - \\nabla_x\\): \\[\\begin{align} d_t \\nabla_{\\dot x} - \\nabla_x &amp;= d_t (J_{\\dot q\\to \\dot x} \\nabla_{\\dot q}) - J_{q\\to x}\\nabla_q - J_{x\\to \\dot q}\\nabla_{\\dot q} \\\\ &amp;= (d_t J_{\\dot q\\to \\dot x})\\nabla_{\\dot q} + J_{\\dot q\\to \\dot x} d_t\\nabla_{\\dot q} - J_{q\\to x}\\nabla_q - J_{q\\to \\dot x}\\nabla_{\\dot q} \\\\ &amp;= (d_tJ_{x\\to q})\\nabla_{\\dot q} + J_{q\\to x} d_t\\nabla_{\\dot q} - J_{q\\to x}\\nabla_q - J_{q\\to \\dot x}\\nabla_{\\dot q} \\\\ &amp;= J_{q\\to \\dot x}\\nabla_{\\dot q} + J_{q\\to x} d_t\\nabla_{\\dot q} - J_{q\\to x}\\nabla_q - J_{q\\to \\dot x}\\nabla_{\\dot q} \\\\ &amp;= J_{q\\to x} d_t\\nabla_{\\dot q} - J_{q\\to x}\\nabla_q \\\\ &amp;= J_{q\\to x}(d_t \\nabla_{\\dot q} - \\nabla_q) \\end{align}\\] We applied lemmas 2.3 and 2.4 on line \\(1\\), applied the product rule to \\(J_{\\dot x\\to \\dot q}\\nabla_{\\dot q}\\) to obtain line \\(2\\) line \\(2\\), then cancelled the dots by lemma 2.1 to obtain line \\(3\\). Finally, we applied lemma 2.2 to line \\(3\\), cancelled terms, and regrouped. Constraints Sometimes it is convenient to consider an over-parameterized change of coordinates \\(x\\to q\\). In this case, \\(J_{q\\to x}\\) is “lean and tall”, while \\(J_{x\\to q}\\) “short and wide.” Recall the covariant formula \\[\\begin{align} \\mathbb L_x &amp;= J_{x\\to q} \\mathbb L_q \\end{align}\\] When \\(J_{x\\to q}\\) is full rank, the physicality condition \\[ \\mathbb L_x = 0 \\iff \\mathbb L_q=0 \\] When \\(q\\) is overparameterized, however, requiring \\(\\mathbb L_q=0\\) is too strong: the correct condition is \\[ \\mathbb L_x = 0 \\iff \\mathbb L_q\\in \\ker J_{x\\to q} \\] Consider a single constraint enforced by some constraint function \\(f(q)=0\\) of signature \\(f:\\mathbb R^n\\to \\mathbb R\\). Proposition 2.1 Given a constraint \\(f:\\mathbb R^N\\to \\mathbb R\\) and a trajectory \\(q(t)\\), if \\(f(q(t))=0\\) for all points on the trajectory, then \\[ \\nabla f \\in \\ker J_{x\\to q} \\] To be precise, the gradient \\((\\nabla f)(q(t))\\) evaluated at all points \\(q(t)\\) for all \\(t\\) is in the kernel of \\(J_{x\\to q}(q(t))\\) evaluated at \\(q(t)\\). Proof: Recognizing the chain rule: \\(J_{x\\to q} \\nabla\\big|_q f = J_{x\\to f}=0\\) since \\(f\\) is constant. Definition 2.3 (complete, independent constraints) Given an over-parameterized system with configuration in \\(\\mathbb R^{m&gt;n}\\), where there are only \\(n\\) true degrees of freedom, a set of constraints \\(\\{f_j:\\mathbb R^m\\to \\mathbb R\\}_{j=1}^k\\) is independent if for all possible configurations \\(q\\in \\mathbb R^m\\), the set of vectors \\[ C(q) = \\{\\nabla f_j\\big|_q\\}_{j=1}^k \\] are linearly independent. They are complete if \\(C(q)\\) spans \\(\\ker J_{x\\to q}(q)\\) for all points \\(q\\). All of this is a fancy way of saying that \\(f\\) is complete and independent iff they provide necessary and sufficient information to discern physicality \\(\\mathbb L_q\\in \\ker J_{x\\to q}\\). Given these constraints, \\(\\mathbb L_x=0\\) iff \\[\\begin{align} \\mathbb L_q(q, \\dot q) &amp;= \\sum_j \\lambda_j \\nabla_q f_j(q) \\in \\ker J_{x\\to q}, \\quad \\lambda_j \\in \\mathbb R\\\\ f_j(q) &amp;= 0 \\end{align}\\] This constrained problem can be solved by the standard Lagrange multiplier method. Extremization formulation In this subsection, we show that the Euler-Lagrange equations are fulfilled if and only if a certain action functional is extremized. This yields a powerful variational perspective on physicality. Definition 2.4 (action functional) Given a Lagrangian \\(\\mathcal L(q, \\dot q, t)\\) (definition 2.1), its associated action functional \\(\\mathcal S_{[a, b]}(p)\\) takes in \\(3\\) arguments: Begining time \\(a\\in \\mathbb R\\). End time \\(b\\in \\mathbb R\\). A path \\(q:[a, b]\\to \\mathbb R^n\\). and outputs the scalar \\[ \\mathcal S_{[a, b]}(p) = \\int_a^b \\mathcal L(q(t), \\dot q(t), t)\\, dt \\] To properly define extremization, we need the directional derivative. Fixing \\(a, b\\), the directional derivative \\((D_\\eta \\mathcal S_{[a, b]})(q)\\) tells us how much \\(\\mathcal S(q)\\) changes if we nudge \\(q\\) in the direction of \\(\\eta\\) infinitesimally. Definition 2.5 (directional derivative of action) Fixing \\(a, b\\), the directional derivative \\((D_\\eta \\mathcal S_{[a, b]})(q)\\) takes in two arguments: The perturbing path \\(\\eta: [a, b]\\to \\mathbb R^n\\). The path at which the perturbation is evaluated \\(q:[a, b]\\to \\mathbb R^n\\). and outputs the scalar slope of \\(\\mathcal S_{[a, b]}(q+\\epsilon \\eta)\\) w.r.t \\(\\epsilon\\), evaluated at \\(0\\): \\[ (D_\\eta \\mathcal S_{[a, b]})(q) = \\lim_{\\epsilon \\to 0} \\dfrac{\\mathcal S_{[a, b]}(q) + \\mathcal S_{[a, b]}(q+\\epsilon \\eta)}{\\epsilon} \\] To reduce notation clutter, we abbreviate this as \\[ D_\\eta S(q) = d_\\epsilon \\big|_0 S(q+\\epsilon \\eta) \\] The magic in the air is that the Euler-Lagrange vector (definition 2.2) \\(\\mathbb L_q\\) gives us the directional derivative when the input perturbation is endpoint-preserving. Theorem 2.2 (endpoint-preserving derivative) For endpoint-preserving \\(\\eta:[a, b]\\to \\mathbb R\\) such that \\(\\eta(a)=\\eta(b)=0\\), the directional derivative of \\(\\mathcal S\\) is given by \\[ (D_\\eta \\mathcal S)(q) = -\\int_a^b \\mathbb L_q(t) \\eta(t)\\, dt \\] Proof: Fixing \\(q, \\eta\\), for any \\(\\epsilon&gt;0\\) let \\(q_\\epsilon:[a, b]\\to \\mathbb R^n\\) denote the family of paths given by \\[ q_\\epsilon(t) = q(t) + \\epsilon \\eta(t) \\tag{2.2} \\] Slide \\(d_\\epsilon\\) into the integral and apply lemma 2.5, then apply the endpoint-vanishing condition \\[\\begin{align} (D_\\eta \\mathcal S)(q) &amp;= d_\\epsilon \\big|_{\\epsilon=0} \\mathcal S(q+\\epsilon \\eta) = \\int_a^b d_\\epsilon \\mathcal L(q_\\epsilon, \\dot q_\\epsilon, t)\\, dt \\\\ &amp;= \\int_a^b \\left[ -\\eta \\cdot \\mathbb L_q + d_t \\left(\\eta \\cdot \\nabla_{\\dot q}\\mathcal L\\right) \\right]\\, dt \\\\ &amp;= \\left(\\eta \\cdot \\nabla_{\\dot q}\\mathcal L\\right)\\big|^b_a -\\int_a^b \\eta \\cdot \\mathbb L_q\\, dt = -\\int_a^b \\eta \\cdot \\mathbb L_q\\, dt \\end{align}\\] The following lemma is the key component in the calculus of variations. Make sure to read it carefully. Lemma 2.5 (ε-derivative of the perturbed Lagrangian) With \\(q_\\epsilon=q + \\epsilon \\eta\\) as in (2.2) \\[ d_\\epsilon \\mathcal L(q_\\epsilon, \\dot q_\\epsilon) = -\\eta \\cdot \\mathbb L_q + d_t \\left(\\eta \\cdot \\nabla_{\\dot q}\\mathcal L\\right) \\] Proof: Using the dependence \\(\\epsilon \\to (q_\\epsilon, \\dot q_\\epsilon) \\to \\mathcal L\\), apply the chain rule \\[\\begin{align} d_\\epsilon \\mathcal L(q_\\epsilon, \\dot q_\\epsilon) &amp;= (d_\\epsilon q_\\epsilon) \\cdot (\\nabla_q \\mathcal L) + (d_\\epsilon \\dot q_\\epsilon) \\cdot (\\nabla_{\\dot q} \\mathcal L) \\\\ &amp;= \\eta \\cdot \\nabla_q \\mathcal L + d_t \\eta \\cdot \\nabla_{\\dot q} \\mathcal L \\\\ &amp;= \\eta \\cdot \\nabla_q \\mathcal L + d_t \\left(\\eta \\cdot \\nabla_{\\dot q}\\mathcal L \\right) - \\eta \\cdot d_t \\nabla_{\\dot q}\\mathcal L \\\\ &amp;= \\eta \\cdot (\\nabla_q - d_t \\nabla_{\\dot q})\\mathcal L + d_t \\left(\\eta \\cdot \\nabla_{\\dot q} \\mathcal L\\right)\\\\ &amp;= -\\eta \\cdot \\mathbb L_q + d_t \\left(\\eta \\cdot \\nabla_{\\dot q}\\mathcal L\\right) \\end{align}\\] On the second line, we used integration by parts on the time-derivative to transfer the time-derivative on \\(\\eta\\) (which we don’t like) to \\(\\nabla_{\\dot q}\\mathcal L\\) (which we like, since it appears in \\(\\mathbb L_q\\)). It is a standard result in real analysis (fundamental lemma of the calculus of variations) that for \\[ (D_\\eta \\mathcal S)(q) = -\\int_a^b \\mathbb L_q(t) \\eta(t)\\, dt \\] to be \\(0\\) for all reasonable endpoint-preserving perturbations \\(\\eta\\) (which includes, in particular, compactly supported smoth functions), \\(\\mathbb L_q(t)\\) must be \\(0\\). This allows us to prove another equivalence result. Theorem 2.3 (Hamilton's principle) Recall that, given a Lagrangian \\(\\mathcal L(q, \\dot q, \\epsilon)\\), we call a path \\(q:[a, b]\\to \\mathbb R^n\\) physical iff \\[ \\mathbb L_q(t)=0, \\quad t\\in [a, b], \\quad \\mathbb L_q(t) = (d_t \\nabla_{\\dot q} - \\nabla_q) \\mathcal L(q(t), \\dot q(t), t) \\] Then a path \\(q\\) is physical iff it extremizes the action functional at \\(\\mathcal S_{[a, b]}(q)\\) (definition 2.4) w.r.t all endpoint-preserving perturbations \\(\\eta:[a, b]\\to \\mathbb R^n, \\eta(a)=\\eta(b)=0\\) \\[ (D_\\eta \\mathcal S)(q) = 0 \\] Representation perspective In this section we give some intuition to the equation \\[ (D_\\eta \\mathcal S)(q) = -\\langle\\mathbb L_q, \\eta\\rangle, \\quad \\langle f, g\\rangle= \\int_a^b f(t)g(t)\\, dt \\tag{2.3} \\] To motivate this, consider a finite-dimensional real vector space \\(V\\) equipped with an inner product \\(\\langle a, b\\rangle\\). A (real) linear functional \\(\\alpha\\) on the space of \\(V\\) is a function of the signature \\(\\alpha:V\\to \\mathbb R\\) satisfying linearity: \\[ \\alpha(u + c v) = \\alpha(u) + c\\alpha(v), \\quad \\forall u, v\\in V, \\quad c\\in \\mathbb R \\] Some thought shows that every linear functional is uniquely specified by another vector \\(a\\in V\\) according to \\(\\alpha(v) = \\langle a, v\\rangle\\). Fix any basis \\(\\{b_1, \\cdots b_n\\}\\), given \\(\\alpha:V\\to \\mathbb R\\) we let \\[ a = \\sum_{j=1}^n \\alpha(b_j) b_j \\implies \\langle a, \\sum_{j=1}^N c_j b_j\\rangle= \\sum c_j \\alpha(b_j) = \\alpha\\left( \\sum_{j=1}^N c_j b_j \\right) \\] Fixing an interval \\(a, b\\) and classical trajectory \\(q\\) on \\(a, b\\), note the following facts: the set of all endpoint-preserving perturbations \\(\\eta:[a, b]\\to \\mathbb R^n\\) form a vector space \\(V\\). The map \\(\\eta\\mapsto (D_\\eta \\mathcal S)(q)\\) is a linear functional w.r.t. \\(\\eta\\): this is the main content of theorem 2.2. With regards to our previous discussion, theorem 2.2 equivalently states that we can regard \\[ -\\mathbb L_q = (\\nabla_q - d_t \\nabla_{\\dot q})\\mathcal L(q, \\dot q) \\] as the representation of the linear functional \\(\\eta \\mapsto (D_\\eta \\mathcal S)(q)\\), just as how \\(a\\in V\\) represents \\(\\alpha:V\\to \\mathbb R\\). Separability of quadratic Lagrangians The following result is also used in the path integral formulation of quantum mechanics. Proposition 2.2 (Actions of quadratic Lagrangians are separable about the classical trajectory) Given a quadratic Lagrangian \\[ \\mathcal L(q, \\dot q) = u\\, \\dot q^2 + v\\, q^2 \\] for path \\(q\\) wich satisfies the Euler-Lagrange equations, the Lagrangian satisfies \\[ \\mathcal L(q+\\eta, \\dot q + \\dot \\eta) = \\mathcal L(q, \\dot q) + \\mathcal L(\\eta, \\dot \\eta) = \\mathcal L(q, \\dot q) + \\epsilon \\mathcal L(\\eta, \\dot \\eta) + d_t \\left[\\eta \\cdot \\nabla_{\\dot q}\\mathcal L(q, \\dot q)\\right] \\] In particular, if the perturbation \\(\\eta:[a, b]\\to \\mathbb R^n\\) is additionally endpoint preserving so \\(\\eta(a)=\\eta(b)=0\\), then \\[ \\mathcal S(q+\\eta) = \\mathcal S(q)+\\mathcal S(\\eta) \\] Proof: Consider the function in \\(\\epsilon:\\mathbb R\\to \\mathbb R\\) defined by \\[\\begin{align} \\mathcal L(\\epsilon) &amp;= \\mathcal L(q + \\epsilon \\eta, \\dot q + \\epsilon \\dot \\eta) = u (\\dot q + \\epsilon \\dot \\eta)^2 + v(q + \\epsilon \\eta)^2 \\\\ \\end{align}\\] It helps at this point to compute some of its derivatives \\[\\begin{align} \\eta \\cdot \\nabla_q \\mathcal L &amp;= \\eta \\cdot 2v\\epsilon (q + \\epsilon \\eta), \\quad \\eta \\cdot \\nabla_{\\dot q} \\mathcal L = \\dot \\eta \\cdot 2u\\epsilon (\\dot q + \\epsilon \\dot \\eta) \\\\ \\dfrac 1 2 (\\eta \\cdot \\nabla_q)^2 \\mathcal L &amp;= (\\eta \\cdot \\nabla_q)\\left[ \\eta \\cdot 2v\\epsilon (q + \\epsilon \\eta) \\right] = \\epsilon v \\, \\eta^2 \\\\ \\dfrac 1 2 (\\eta \\cdot \\nabla_{\\dot q})^2 \\mathcal L &amp;= (\\dot \\eta \\cdot \\nabla_{\\dot q}) \\left[\\dot \\eta \\cdot 2u\\epsilon (\\dot q + \\epsilon \\dot \\eta) \\right] = \\epsilon u \\, \\dot \\eta^2 \\\\ [(\\nabla \\cdot \\nabla_q)(\\dot \\eta \\cdot \\nabla_{\\dot q})] \\mathcal L &amp;= [(\\dot \\eta \\cdot \\nabla_{\\dot q})(\\nabla \\cdot \\nabla_q)] \\mathcal L = 0 \\\\ \\dfrac 1 2 (\\eta \\cdot \\nabla_q + \\dot \\eta \\cdot \\nabla_{\\dot q})^2 \\mathcal L(\\epsilon) &amp;= \\epsilon \\mathcal L(\\eta, \\dot \\eta) \\end{align}\\] Taylor expanding about \\(\\epsilon=0\\) yields \\[\\begin{align} \\mathcal L(q + \\epsilon \\eta, \\dot q + \\epsilon \\dot \\eta) &amp;= \\mathcal L(q, \\dot q) + \\sum_{k=1}^\\infty \\dfrac{ (\\eta \\cdot \\nabla_q + \\dot \\eta \\cdot \\nabla_{\\dot q})^k } {k!} \\mathcal L(q + \\epsilon \\eta, \\dot q + \\epsilon \\dot \\eta) \\\\ &amp;= \\mathcal L(0) + (\\eta \\cdot \\nabla_q + \\dot \\eta \\cdot \\nabla_{\\dot q}) \\mathcal L + \\dfrac 1 2 (\\eta \\cdot \\nabla_q + \\dot \\eta \\cdot \\nabla_{\\dot q})^2 \\mathcal L \\\\ &amp;= \\mathcal L(q, \\dot q) + \\epsilon \\mathcal L(\\eta, \\dot \\eta) + (\\eta \\cdot \\nabla_q + \\dot \\eta \\cdot \\nabla_{\\dot q}) \\mathcal L \\tag{2.4} \\end{align}\\] We will now show that the extra last term (which came from the linear expansion) is zero when \\(\\eta\\) is time-dependent. We do this by using the product rule and the condition that at \\(\\epsilon=0\\), the path satisfies the Euler-Lagrange equation \\[\\begin{align} (\\eta \\cdot \\nabla_q + \\dot \\eta \\cdot \\nabla_{\\dot q})\\mathcal L &amp;= [\\eta \\cdot \\nabla_q + d_t(\\eta \\cdot \\nabla_{\\dot q}) - \\eta \\cdot d_t \\nabla_{\\dot q}] \\mathcal L \\\\ &amp;= d_t(\\eta \\cdot \\nabla_{\\dot q})\\mathcal L \\end{align}\\] Where we recognized \\(\\eta \\cdot \\nabla_q - \\eta \\cdot (d_t\\nabla_{\\dot q})\\mathcal L = -\\eta \\cdot \\mathbb L = 0\\). Substituting back into equation (2.4) \\[ \\mathcal L(q + \\epsilon \\eta, \\dot q + \\epsilon \\dot \\eta) = \\mathcal L(q, \\dot q) + \\epsilon \\mathcal L(\\eta, \\dot \\eta) + d_t \\left[\\eta \\cdot \\nabla_{\\dot q}\\mathcal L(q, \\dot q)\\right] \\] Substitute \\(\\epsilon=1\\) and integrate to obtain the action. The last total-derivative term vanishes by endpoint-preserving property of \\(\\eta\\). "],["noethers-theorem.html", "3 Noether’s theorem", " 3 Noether’s theorem Noether’s theorem elucidates how continuous symmetries result in conserved quantities; the converse will be provided by Hamiltonian mechanics. The key points in this section are: The definition of a continuous symmetry. Noether’s theorem. The conserved-quantity when \\(\\mathcal L\\) is time-invariant: the Hamiltonian. It’s convenient to now define a quantity we’ll be using later very often: Definition 3.1 (canonical momentum) Given the Lagrangian \\(\\mathcal L(q, \\dot q, t)\\) of a system, its canonical momentum is \\[ p = \\nabla_{\\dot q} \\mathcal L(q, \\dot q, t) \\] In terms of the canonical momentum, the Euler-Lagrange equations read \\[ \\mathbb L_q = \\dot p - \\nabla_q \\mathcal L = 0 \\] We will make extensive use of lemma 2.5, which we state here again using the canonical momentum for convenience: Lemma 3.1 (ε-derivative of the perturbed Lagrangian) Given a path \\(q:[a, b]\\to \\mathbb R\\) and a perturbation path \\(\\eta:[a, b]\\to \\mathbb R\\), define the parameterized family of paths \\(q_\\epsilon=q + \\epsilon \\eta\\), then \\[ d_\\epsilon \\mathcal L(q_\\epsilon, \\dot q_\\epsilon) = -\\eta \\cdot \\mathbb L_q + d_t \\left(\\eta \\cdot p\\right) \\] Definition 3.2 (continuous symmetry) Consider a family of maps \\(q_\\epsilon\\) such that \\(q_0\\) is the classical trajectory w.r.t. the Lagrangian \\(\\mathcal L\\). Define \\[ \\mathcal L_\\epsilon = \\mathcal L(q_\\epsilon, \\dot q_\\epsilon, t) \\] as well as the parameterized action function \\[ S_\\epsilon = \\int_a^b \\mathcal L(q_\\epsilon, \\dot q_\\epsilon, t)\\, dt \\] The physical system has continuous symmetry under the transformation parameterized by \\(q\\mapsto q_\\epsilon\\) if \\[ d_\\epsilon \\big|_0 S_\\epsilon = C \\] where \\(C\\) is a constant independent of the dynamical variables \\(q, \\dot q, t\\). In other words, infinitesimal reparameterization by \\(\\epsilon\\) leaves the equations of motion unchanged. Proposition 3.1 (characterization of continuous symmetry via Lagrangian) \\(\\epsilon \\mapsto q_\\epsilon\\) is a continuous symmetry if \\[ d_\\epsilon \\big|_0 \\mathcal L_\\epsilon = d_t F(q, \\dot q, t) \\] Note that the right-hand side must be a total time derivative. Proof: Direct computation: \\[ d_\\epsilon \\big|_0 S_\\epsilon = \\int_a^b d_\\epsilon \\big|_0 \\mathcal L_\\epsilon \\, dt = \\int_a^b d_t(F) \\, dt = F(b)-F(a) \\] Theorem 3.1 (Noether's theorem) Given a continuous symmetry \\(q_\\epsilon\\) such that \\[ d_\\epsilon \\big|_0 \\mathcal L(q_\\epsilon, \\dot q_\\epsilon, t) = d_t F(q, \\dot q, t) \\] The following Noether change is conserved along a classical trajectory obeying the Euler-Lagrange equations \\[ Q = \\left(d_\\epsilon \\big|_0 q_\\epsilon \\right) \\cdot \\nabla_{\\dot q}\\mathcal L(q, \\dot q, t) - F(q, \\dot q, t) \\] Let \\(q_\\epsilon = q + \\epsilon \\eta\\) for small \\(\\epsilon\\) (this is effectively adapting vector field perspective), the Noether charge is \\[ Q = \\eta \\cdot p - F(q, \\dot q, t) \\] One special case is for \\(F\\) to vanish, in which case \\[ d_\\epsilon \\big|_0 \\mathcal L(q_\\epsilon, \\dot q_\\epsilon, t) = 0 \\implies Q = \\eta \\cdot p \\text{ conserved.} \\] Proof: Direct computation invoking lemma 3.1: the \\(\\mathbb L_q\\) term vanishes on the classical trajectory \\[\\begin{align} d_t F &amp;= d_\\epsilon \\big|_0 \\mathcal L(q_\\epsilon, \\dot q_\\epsilon) = - \\eta \\cdot \\mathbb L_q + d_t(\\eta \\cdot p) = d_t(\\eta \\cdot p) \\end{align}\\] This implies that the time-derivative of the Noether charge is \\(0\\). Example 3.1 (the Hamiltonian) Let \\(\\mathcal L\\) be independent of time and denote the time-translation transform \\[ q_\\epsilon(t) = q(t+\\epsilon) \\] This is a continuous symmetry with \\(F(q, \\dot q) = \\mathcal L(q, \\dot q)\\) \\[ d_\\epsilon \\big|_0 \\mathcal L(q_\\epsilon, \\dot q_\\epsilon) = d_t \\mathcal L \\] The associated \\(\\eta = d_\\epsilon \\big|_0 q_\\epsilon = d_\\epsilon \\big|_0 q(t+\\epsilon) = \\dot q\\). Substituting into Noether’s theorem yields the conserved quantity \\[ H = \\dot q\\cdot p - \\mathcal L(q, \\dot q) \\] "],["hamiltonian-mechanics.html", "4 Hamiltonian Mechanics Legendre transform Hamilton’s equations The Poisson bracket Liouville’s theorem Optional: probability on phase space", " 4 Hamiltonian Mechanics We begin by exporing the properties of the Legendre transform, which relates (convex) Lagrangians and Hamiltonians. We next introduce Hamilton’s equations, with some optional perspective from differential geometry which yield some of the most mathematically satisfactory / beautiful treatment of the topic. Key takeaways of this part are: The Hamiltonian \\(H(q, p, t)\\) is the Legendre transform of \\(\\mathcal L(q, \\dot q, t)\\) fixing \\(q, t\\); fixing \\(q, t\\), the conjugate variables \\(\\dot q\\) and \\(p\\) are functions of each other by \\(\\nabla_{\\dot q} \\mathcal L = p\\) and \\(\\dot q = \\nabla_p H\\) (lemma 4.1). The Jacobian \\(J_{p\\to q}\\) is the Hessian. When the \\(\\sup\\) is dropped in the Legendre transform, the two variables are always understood to obey an implicit equation. On strictly convex functions, the Legendre transform preserves convexity, and is involutary. The scalar Hamiltonian function \\(H(q, p, t)\\in \\mathbb R\\) generates the flow of time in phase space, or time-translation symmetry. Converse to Noether’s theorem: every scalar function on phase space generates a flow corresponding to its continuous symmetry: Legendre transform The Legendre transform is a general transform of functions which interactive particularly well with convex functions. Definition 4.1 (Legendre transform) Given a function \\(f:\\mathbb R^n\\to \\mathbb R\\), its Legendre transform \\(\\mathbb L f: \\mathbb R^n\\to \\mathbb R\\) is \\[ (\\mathbb L f)(p) = \\sup_x \\, \\langle x, p\\rangle- f(x) \\] Convex functions Convex functions are characterized by \\[ f(\\lambda x + \\bar \\lambda y) \\leq \\lambda f(x) + \\bar \\lambda f(y), \\quad \\lambda \\in [0, 1], \\bar \\lambda = 1 - \\lambda \\] assume they’re well-behaved enough, they’re equivalently defined by having positive (semi) definite second-derivatives (Hessian) at all points: \\[ (\\mathcal H_x f)(\\forall x) \\geq 0, \\quad (\\mathcal H_x f)_{ij}(x) = (\\partial_{x_i x_j}^2 f)(x) \\] Here \\(A\\geq 0\\) for matrix \\(A\\) is understood as all eigenvalues of \\(A\\) being nonnegative. Convexity is further equivalent to \\[ \\forall x, y: f(y) \\geq \\nabla f(x) \\cdot (y - x) \\] The function \\(f\\) is strictly convex when the inequalities are strict. Proposition 4.1 The gradient map \\(x\\mapsto (\\nabla f)(x)\\) is invertible if \\(f\\) is strictly convex. Proof: For \\(x\\neq y\\), strict convexity implies \\[\\begin{align} \\nabla f(x) \\cdot (y-x) &lt; f(y) - f(x), \\quad \\nabla f(y) \\cdot (x-y) &lt; f(x) - f(y) \\end{align}\\] Suppose for contradiction that \\(\\nabla f(x) = \\nabla f(y)\\), substituting \\(\\nabla f(y)=\\nabla f(x)\\) in the last equation and multiplying both sides by \\(-1\\) yields the contradiction \\[ \\nabla f(x)\\cdot (y-x) &gt; f(y) - f(x) \\] Properties of the Legendre transform We now show that the Legendre transform is well-behaved for strictly convex functions. Assuming \\(f\\) well-behaved, the extremality condition is \\[ \\nabla_x \\langle x, p\\rangle- f(x) = p - \\nabla_x f(x) = 0 \\iff p = \\nabla_x f \\] This means that the \\(\\sup\\) may be dropped when it is understood that \\(x\\) is an implict function of \\(p\\) via the equation \\(p = \\nabla_x f\\) : \\[ (\\mathbb L f)(p=\\nabla_x f) = x\\cdot p - f(x) \\] The following lemma will prove the Hamilton equation \\(\\dot q = \\nabla_p H\\). Lemma 4.1 (two-way relation between x and p) Given a Legendre transform \\[ g(p) = (\\mathbb L f)(p) = \\sup_x p\\cdot x - f(x) \\] Then \\(p = \\nabla_x f \\iff x = \\nabla_p g\\). Proof: Assuming \\(p = \\nabla_x f\\), then we can drop the \\(\\sup\\) in the Legendre transform, yielding \\[ g(p) = p\\cdot x - f(x) \\] Take the gradient w.r.t. \\(p\\) on both sides; note that we need to invoke the chain rule since \\(p, x\\) are related: \\[\\begin{align} \\nabla_p g(p) &amp;= \\nabla_p (p\\cdot x - f(x)) = x + J_{p\\to x} p - J_{p\\to x} (\\nabla_x f)_{=p} = x \\end{align}\\] Proposition 4.2 (Jacobian of Legendre transform) The Jacobian of the Legendre transform \\(x\\to p\\) is the Hessian of \\(f\\) \\[ J_{x\\to p} = d_x p = d_x \\left(\\nabla_x f\\right) = \\mathcal H_x f \\] Proposition 4.3 (convexity invariance) Suppose \\(f(x)\\) is convex strictly convex so \\(\\mathcal H_x f&gt; 0\\) (positive-definite) everywhere, then \\(\\mathbb L f\\) is convex, i.e. \\(\\mathcal H_p (\\mathbb L f)&gt;0\\). Proof: Computing the Hessian explicitly and recognize \\(\\nabla_x f = p\\) \\[\\begin{align} \\mathcal H_p(\\mathbb L f) &amp;= d_p [\\nabla_p (\\mathbb L f)] = d_p \\left[ x + J_{p\\to x} p - J_{p\\to x}\\nabla_x f \\right] \\\\ &amp;= d_p x = J_{p\\to x} = J_{x\\to p}^{-1} = \\left(\\mathcal H_x f\\right)^{-1} \\end{align}\\] Convexity follows from the convexity of \\(\\mathcal H_x f&gt; 0\\). Proposition 4.4 (involutary) For convex \\(f\\), the Legendre transform is involutary \\[ \\mathbb L \\left[(\\mathbb L f)(p)\\right] (x) = f(x) \\] Proof: Unroll the definition \\[\\begin{align} \\mathbb L \\left[(\\mathbb L f)(p)\\right] (x) &amp;= \\mathbb L \\left[p\\mapsto xp - f(x)\\right] (x)\\\\ &amp;= xp - \\left[p\\mapsto xp - f(x)\\right]\\, p = xp - xp - f(x) = f(x) \\end{align}\\] Note the dependence here: fixing \\(x\\), we can fix \\(p=\\nabla_x f\\), which in turn determines \\(x\\) in the inner Legendre transform. Hamilton’s equations Definition 4.2 (Hamiltonian) Given the Lagrangian \\(\\mathcal L(q, \\dot q, t)\\), the Hamiltonian is the Legendre transform of \\(\\dot q\\mapsto p\\) holding \\(q\\) fixed \\[ H(q, p, t) = p\\cdot \\dot q - \\mathcal L(q, \\dot q(p), t) \\] where \\(p = \\nabla_{\\dot q} \\mathcal L\\) and \\(\\dot q(p)\\) is the implicit inverse of this equation. Theorem 4.1 (Hamilton's equations) The following equations are equivalent to the Euler-Lagrange equations 2.2 when the Lagrangian \\(\\mathcal L(q, \\dot q, t)\\) is strictly convex in in \\(\\dot q\\): \\[\\begin{aligned} \\dot p = -\\nabla_q H, \\quad \\dot q = \\nabla_p H \\end{aligned}\\] Collecting \\(\\xi = q \\oplus p\\) and write \\(H(q, p, t) = H(\\xi, t)\\), we have the compact expression \\[ \\partial_{t} \\xi = \\Gamma\\, \\nabla_\\xi H, \\quad \\Gamma = \\bigoplus \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\] Proof: Take care that \\(\\dot q, p\\) are implicit functions of each other and recognize \\(p=\\nabla_{\\dot q}\\mathcal L\\); holding \\(q\\) fixed: \\[\\begin{align} \\nabla_p H(q, p, t) &amp;= \\nabla_p \\left[p\\cdot \\dot q - \\mathcal L(q, \\dot q(p), t)\\right] \\\\ &amp;= \\dot q + J_{p\\to \\dot q} p - J_{p\\to \\dot q}(\\nabla_{\\dot q}\\mathcal L) = \\dot q \\end{align}\\] For the second equation, we’re holding \\(p\\) fixed, but \\(\\dot q\\) is still dependent upon \\(q\\), then \\[\\begin{align} \\nabla_q H(q, p, t) &amp;= \\nabla_q \\left[ p\\cdot \\dot q - \\mathcal L(q, \\dot q(p), t) \\right] \\\\ &amp;= J_{q\\to \\dot q}p - \\nabla_q \\mathcal L - J_{q\\to \\dot q} (\\nabla_{\\dot q}\\mathcal L) = - \\nabla_q \\mathcal L \\\\ \\dot p &amp;= d_t \\nabla_{\\dot q}\\mathcal L = \\nabla_q \\mathcal L \\end{align}\\] the last equation holds by the Euler-Lagrange equations. Remark. For those interested in differential geometry, Hamilton’s equations \\(\\partial_{t} \\xi = \\Gamma \\, \\nabla_\\xi H\\) is the component representation of the following implicit equation which defines the vector field \\(X_H\\) generating the time-translation of the system: \\[ \\omega(X_H, \\, \\cdot\\, ) = dH \\] Here \\(\\omega= dq \\wedge dp\\) is the symplectic form represented by \\(\\Gamma\\), and \\(dH\\) is the differential of \\(H\\). optional material: relation between implicit and coordinate equations; differential geometry Given a chart \\(x=(x_1, \\cdots, x_n)\\) on a manifold \\(M\\) (like fixing a basis), a vector field \\(X\\) can be understood as a directional derivative operation that consumes a scalar function \\(f\\) and outputs \\(X\\, f\\) (read as \\(X\\) acting on \\(f\\). The value of \\((X\\, f)(x)\\) encodes how much \\(f\\) changes along the direction of the vector field \\(X\\) at location \\(x\\). The representation of \\(X\\) in this chart \\(x\\) is then \\[ X = \\sum_{j=1}^n X_j \\partial_{j} \\] where each \\(X_j\\) is a scalar function and \\(\\partial_{j} = \\partial_{x_j}\\) denotes the partial differentiation operation in the \\(j\\)-th variable; in this representation, the action of \\(X\\) can be locally written in coordinate form as \\[ (X\\, f)(x) = \\sum_{j=1}^n X_j \\partial_{j} f(x) = (X_j)\\cdot \\nabla f(x) \\] Here \\((X_j)\\) is understood as a vector denoting the “direction”; in this sense the chart \\(x\\) also fixes a basis \\(\\partial_{1}, \\cdots \\partial_{n}\\) for the space of vector fields (tangent bundle). Given a scalar function \\(f\\), its differential \\(df\\) consumes a vector field and outputs a scalar (function) \\[ df\\, X = X\\, f \\] We can similarly consider a basis representation for the differentials (which live on the cotangent bundle) with the notation \\(d x_1, \\cdots, dx_n\\) \\[ df = \\sum_{j=1}^N f_j dx_j, \\quad dx_j\\, \\partial_{i} = \\delta_{ij} \\] The last piece we need is the wedge product: assuming a basis \\(B=\\{\\partial_{p}, \\partial_{q}\\}\\) (\\(p\\), \\(q\\) are univariables), the symplectic form \\(\\omega = dq \\wedge dp\\) consumes two arguments in \\(B\\) and outputs a number subject to the following rules, in addition to being linear in all its arguments: \\[ (dq \\wedge dp)(\\partial_{a}, \\partial_{b}) = \\begin{cases} 1 &amp; (\\partial_{a}, \\partial_{b}) = \\partial_{q}, \\partial_{p} \\\\ -1 &amp; (\\partial_{a}, \\partial_{b}) = \\partial_{p}, \\partial_{q} \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] Back to Hamilton’s equation \\(\\omega(X_H, \\cdot) = dH\\). Fix the basis \\(\\{\\partial_{q}, \\partial_{p}\\}\\) (the order matters!) and consider two vector fields \\(X, Y\\) with components \\[ X = X_q \\partial_{q} + X_p \\partial_{p}, \\quad Y = Y_q \\partial_{q} + Y_p \\partial_{p} \\] The representation \\(\\Gamma\\) of \\(\\omega\\) is read from the equation \\[\\begin{align} \\omega(X, Y) &amp;= X_qY_q \\omega(\\partial_{q}, \\partial_{q})_{=0} + X_p Y_q \\omega(\\partial_{p}, \\partial_{q})_{=-1} + X_qY_p \\omega (\\partial_{q}, \\partial_{p})_{=1} + X_pY_p \\omega (\\partial_{p}, \\partial_{p})_{=0} \\\\ &amp;= X_qY_p - X_p Y_q = \\begin{pmatrix} X_q \\\\ X_p \\end{pmatrix}^T \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} Y_q \\\\ Y_p \\end{pmatrix} \\implies \\Gamma = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} \\end{align}\\] The implicit equation holds for all vector fields \\(Y\\) (since both sides of the equation consume a vector field and outputs a scalar function) \\[ \\omega (X_H, Y) = dH\\, Y = Y\\, H \\implies (X_H)^T \\Gamma Y = Y_q \\partial_{q} H + Y_p \\partial_{p} H = (\\nabla H)^T Y, \\quad \\forall Y \\] where we have identified \\(X_H, Y\\) with their \\(\\partial_{q}, \\partial_{p}\\) coordinate representations on the RHS. Solving for \\(X_H\\) yields \\[ (X_H)^T \\Gamma = (\\nabla H)^T \\iff \\nabla H = ((X_H)^T \\Gamma)^T = -\\Gamma X_H \\iff X_H = \\Gamma \\nabla_H \\] This is exactly the component expression of Hamilton’s equations we have derived. The Poisson bracket Liouville’s theorem Optional: probability on phase space "],["spacetime-geometry.html", "5 Spacetime geometry Matrix representation of spacetime The Lorentz group Boosts and rotations", " 5 Spacetime geometry We introduce the Minkowski metric and the Lorentz group, and show that the algebra is neatly represented by \\(2\\times 2\\) complex matrices. Key takeaways: The Pauli matrices form a basis for \\(2\\times 2\\) complex Hermitian matrices (proposition 5.2). Representation of \\((3+1)\\)-spacetime using Hermitian-matrixs (definition 5.2). In this representation, determinant (of the \\(2\\times 2\\) Hermitian matrix) correspond to the Minkowski norm in \\(\\mathbb R^4\\) (proposition 5.3). Conjugation in the representation corresponds to a linear transformation in spacetime (theorem 5.1). We are interested in the Lorentz group (definition 5.4) which preserve the Minkowski norm. Such (linear) Lorentz transforms are represented the conjugate action of \\(\\mathrm{SL}(2, \\mathbb C)\\), or \\(2\\times 2\\) complex matrices with determinant 1 (proposition 5.4). Rotations are represented by unitary conjugation \\(SU(2)\\) (theorem 5.2). Lorentz boosts are represented by Hermitian conjugation \\(\\mathcal H_2\\cap \\mathrm{SL}(2)\\) (theorem 5.3). In this section, we work with \\(2\\times 2\\) complex matrices. Some terminology for reference: \\(\\mathrm{GL}(2, \\mathbb C)\\) denotes the set of \\(2\\times 2\\) complex invertible matrices. \\(\\mathrm{SL}(2, \\mathbb C)\\subsetneq \\mathrm{SL}(2, \\mathbb C)\\) denotes the set of \\(2\\times 2\\) matrices with \\(\\det A=1\\). \\(U(2)\\) denotes the set of \\(2\\times 2\\) unitary matrices satisfying \\(AA^\\dagger= A^\\dagger A = \\mathbf 1\\). The intersection \\(U(2)\\cap \\mathrm{SL}(2, \\mathbb C)\\) is denoted \\(SU(2)\\), or the special unitary group. \\(\\mathcal H_2\\) denotes the set of \\(2\\times 2\\) Hermitian matrices satisfying \\(A^\\dagger= A\\), where \\(A^\\dagger\\) is the conjugate transpose. Matrix representation of spacetime In this section, we define a representation of spacetime via a bijection between \\(\\mathbb R^4\\) and \\(\\mathcal H_2\\) (definition 5.2) and introduce the Minkowski metric 5.3. the space \\(\\mathbb R^4\\cong \\mathbb R\\times \\mathbb R^3\\) will be understood as \\(4\\)-dimensional spacetime. Both unitary matrices and Hermitian matrices are normal thus subject to the spectral theorem, so one may think of them as a diagonal matrix of eigenvalues in some orthonormal basis. In particular: Hermitian matrices have real eigenvalues. Unitary matrices have complex eigenvalues with unit norm of the form \\(e^{i\\theta}\\). An immediate corollary of the observation above is Proposition 5.1 Every unitary \\(U=\\exp(iH)\\) is the complex exponential of some Hermitian matrix \\(H\\). Proceeding, recall the Pauli matrices \\[ \\mathbf 1 = \\begin{bmatrix} 1 \\\\ &amp; 1\\end{bmatrix}, \\quad \\sigma_x = \\begin{bmatrix} &amp; 1 \\\\ 1 \\end{bmatrix}, \\quad \\sigma_y = \\begin{bmatrix} &amp; -i \\\\ i \\end{bmatrix}, \\quad \\sigma_z = \\begin{bmatrix} 1 \\\\ &amp; -1\\end{bmatrix} \\] The space \\(\\mathcal H_2\\) of \\(2\\times 2\\) Hermitian matrices is a vector space, and it comes equipped with the following inner product: Definition 5.1 (Hilbert-Schmidt inner product) The Hilbert-Schmidt inner product defined on the space of linear operators over a finite-dimensional Hilbert space of dimension \\(d\\) is \\[ \\langle A, B\\rangle= \\dfrac 1 d \\mathrm{Tr}(A^\\dagger B) \\] In a basis representation, this corresponds to the flattened vector inner product. The Pauli matrices are special because they form an orthonormal basis for \\(\\mathcal H_2\\): Proposition 5.2 The Pauli matrices are involutary \\(\\sigma_i^2 = \\mathbf 1\\) and form an orthonormal basis for \\(\\mathcal H_2\\) under the Hilbert-Schmidt inner product. Proof: Direct computation, one can verify. Definition 5.2 (representation of spacetime) We identify points in \\(\\mathbb R^4\\) \\[ x=(x_t, x_1, x_2, x_3)\\in \\mathbb R^4 \\] with \\(2\\times 2\\) Hermitian matrices \\[ \\hat x = x_t\\mathbf 1 + x_1\\sigma_x + x_2\\sigma_y + x_3\\sigma_3 = \\begin{bmatrix} x_t + x_3 &amp; x_1 - i x_2 \\\\ x_1 + ix_2 &amp; x_t - x_3 \\end{bmatrix}\\in \\mathcal H_2 \\] The representation is invertible by Fourier decomposition onto the Pauli basis using 5.1: \\[ x_t = \\langle\\hat x, \\mathbf 1\\rangle= \\dfrac 1 2 \\mathrm{tr}(\\hat x\\mathbf 1), \\quad x_{i\\in \\{1, 2, 3\\}} = \\langle\\hat x, \\sigma_i\\rangle= \\dfrac 1 2 \\mathrm{tr}(\\hat x\\sigma_i) \\] Definition 5.3 (Minkowski metric) The Minkowski metric over \\(\\mathbb R^4\\) is the bilinear map \\(\\langle\\, \\cdot\\, , \\, \\cdot\\, \\rangle_M:\\mathbb R^4\\times \\mathbb R^4\\to \\mathbb R\\). \\[ \\langle x, y\\rangle_M = x_ty_t - x_1y_1 - x_2y_2 - x_3y_3 \\] We denote the Minkowski norm it induces by \\(\\|\\, \\cdot\\|^2_M:\\mathbb R^4\\to \\mathbb R\\), defined by \\(\\|x\\|_M^2 = \\langle x, x\\rangle_M\\). One purpose of the definition 5.2 is the following Proposition 5.3 Under the Pauli identification of \\(\\mathbb R^4\\) \\[ \\det \\hat x = \\langle x, x\\rangle_M \\] Proof: Direct computation: \\(\\det \\hat x = x_t^2 - x_1^2 - x_2^2 - x_3^2\\). The Lorentz group We wish to investigate linear transformations on \\(\\mathbb R^4\\) which leave the Minkowski metric invariant, since these transformations form the Lorentz group. Definition 5.4 (Lorentz group) The Lorentz group \\(\\mathcal L\\) is the subgroup of \\(\\mathrm{GL}(4, \\mathbb R)\\) (group of invertible linear transformations on \\(\\mathbb R^4\\)) which preserve the Minkowski metric \\[ A\\in \\mathcal L\\iff \\forall x, y\\in \\mathbb R^4, \\langle x, y\\rangle_M = \\langle Ax, Ay\\rangle \\iff \\forall x\\in \\mathbb R^4, \\|x\\|^2_M = \\|Ax\\|^2_M \\] Recall that inverse-conjugating any matrix \\(A\\mapsto BAB^{-1}\\) will not change the eigenvalues of \\(A\\). In particular, inverse-conjugating \\(\\hat x\\in \\mathcal H_2\\) by any \\(2\\times 2\\) invertible matrix yields another element of \\(\\mathcal H_2\\); since \\(\\mathcal H_2\\) represent points in spacetime, it is natural to ask the \\(H\\mapsto BHB^{-1}\\) induces on \\(\\mathbb R^4\\). Theorem 5.1 (conjugation-induced linear transform) conjugation in \\(\\mathcal H_2\\) yields a linear transformation in \\(\\mathbb R^4\\): Equivalently, the map \\(A:\\mathbb R^4\\to \\mathbb R^4\\) which makes the diagram below commute is a linear operator. In particular, the representation of \\(\\hat A\\) as a matrix in \\(\\mathbb R^4\\) is \\[ A_{ij} = (Ae_j)_i = \\langle\\hat A \\sigma_j \\hat A^\\dagger, \\sigma_i \\rangle = \\dfrac 1 2 \\mathrm{tr}\\left(\\hat A \\sigma_j \\hat A^\\dagger\\sigma_i\\right) \\tag{5.1} \\] Proof: Fixing \\(\\hat A:\\mathcal C^2\\to \\mathcal C^2\\) and let \\(x, y\\in \\mathbb R^4\\). Let \\(\\sigma_i\\) be any Pauli basis matrix, we show that \\(A\\) is linear: \\[\\begin{align} \\left[A(x+\\alpha y)\\right]_i &amp;= \\langle\\hat A(\\hat x + \\alpha \\hat y)\\hat A^\\dagger, \\sigma_i\\rangle = \\dfrac 1 2 \\mathrm{tr}\\left(\\hat A (\\hat x + \\alpha \\hat y)\\hat A^\\dagger\\sigma_i \\right) \\\\ &amp;= \\dfrac 1 2 \\mathrm{tr}\\left(\\hat A \\hat x \\hat A^\\dagger\\sigma_i \\right) + \\dfrac 1 2 \\alpha \\mathrm{tr}\\left(\\hat A \\hat x \\hat A^\\dagger\\sigma_i \\right) = (Ax)_i + \\alpha(Ay)_i \\end{align}\\] We are interested in the Lorentz group, i.e. induced transformations \\(A:\\mathbb R^4\\to \\mathbb R^4\\) which preserve the Minkowski metric. The relation between Minkowski norm and determinant of the Pauli identification in proposition 5.3 gives a convenient characterization. Proposition 5.4 Conjugation by \\(\\hat A:\\mathbb C^2\\to \\mathbb C^2\\) induce a Lorentz transformation only if \\[ \\det \\hat A = \\pm 1 \\] Moreover, \\(\\hat A\\) induces the same transformation as \\(-\\hat A\\), so without of loss of generality we can take \\(\\det \\hat A = 1\\). Proof: Use proposition 5.3: \\(\\|Ax\\|^2_M = \\det(\\hat A\\hat x\\hat A^\\dagger) = \\det \\hat x (\\det A)^2 = (\\det A)^2 \\|x\\|^2_M\\) Within \\(\\mathrm{SL}(2, \\mathbb C)\\), the special unitary group \\(\\mathrm{SU}(2)\\) consisting of \\(2\\times 2\\) unitaries and the unit-determinant Hermitians \\(\\mathcal H_2\\cap \\mathrm{SL}(2, \\mathbb C)\\) are special: every \\(A\\in \\mathrm{SL}(2, \\mathbb C)\\) may be decomposed into a unique product of \\(U\\in \\mathrm{SU}(2)\\) and \\(H\\in \\mathcal H_2\\cap \\mathrm{SL}(2, \\mathbb C)\\). Boosts and rotations We explore \\(\\mathbb R^4\\) transforms induced by the unitaries and unit-determinant Hermitians, beginning with the unitaries. Recall that every unitary \\(U=\\exp(iH)\\) for some Hermitian \\(H\\). Since the Paulis form a basis, every unitary \\(U\\) may be written uniquely as \\[ U=\\exp\\left[i(x_t \\mathbf 1 + x_1\\sigma_x + x_2\\sigma_y + x_3\\sigma_z)\\right] = \\exp\\left[ix_t + i(x_1\\sigma_x + x_2\\sigma_y + x_3\\sigma_z)\\right] \\] Inspecting equation (5.1) shows that the induced transformation is invariant in \\(x_t\\), and, in fact, an overall scaling of \\((x_1, x_2, x_3)\\). Without loss of generality we consider unitaries of the form \\(U=\\exp(i(\\hat r\\cdot \\sigma))\\), for \\(r\\in S^2\\), the unit \\(2\\)-sphere in \\(\\mathbb R^3\\). Theorem 5.2 (unitaries induce spatial rotations) A clockwise rotation \\(A\\) in the last three spatial coordinates about \\(\\hat r\\in S^2\\) by \\(\\theta\\) is induced via equation (5.1) by \\[ U(\\hat r, \\theta) = \\exp\\left[-\\dfrac i 2 \\theta (\\hat r\\cdot \\vec \\sigma)\\right] \\in \\mathrm{SU}(2) \\] Proof: All conjugation fixes the time axis since \\(\\mathbf 1\\) commutes with everything so \\(U(x_t\\mathbf 1)U^\\dagger= x_t\\mathbf 1\\). The only Minkowski-norm preserving isometries of \\(\\mathbb R^4\\) which fix time are the spatial rotations. The direct form follows by direct computation or noting the identification with quaterions. We do one example here, let \\[ \\hat r = (0, 0, 1), \\quad \\hat R_z(\\theta) = \\exp\\left(-\\dfrac i 2 \\theta\\sigma_z\\right) \\implies R_z(\\theta)_{ij} = \\dfrac 1 2 \\mathrm{tr}\\left(\\hat R_z(\\theta)\\sigma_j \\hat R_z(-\\theta)\\sigma_i \\right) \\] If \\(j=3\\) (corresponding to spatial \\(z\\)) or \\(j=0\\) (time), \\(\\sigma_j\\) commutes with \\(\\hat R_z(\\theta)\\) so \\[ R_z(\\theta)_{ij\\notin\\{0, 3\\}}=\\delta_{ij} \\] This implies that \\(R_z(\\theta)\\) acts trivially on the time and \\(z\\)-axes. Next, let us consider the subspaces spanned by \\(\\sigma_1, \\sigma_2\\). Recall that \\([\\sigma_3, \\sigma_1] = -[\\sigma_1, \\sigma_3]\\) \\[\\begin{align} R_z(\\theta)_{11} &amp;= \\dfrac 1 2 \\mathrm{tr}\\left(e^{-i\\theta \\sigma_3/2}\\sigma_1 e^{i\\theta \\sigma_3/2}\\sigma_1 \\right) \\\\ &amp;= \\dfrac 1 2 \\mathrm{tr}\\left(e^{-i\\theta \\sigma_3}\\sigma_1\\sigma_1 \\right) = \\dfrac 1 2 \\mathrm{tr}\\left(e^{-i\\theta \\sigma_3}\\right) \\\\ &amp;= \\dfrac{e^{i\\theta} + e^{-i\\theta}} 2 = \\cos\\theta \\end{align}\\] The same applies to \\(R_z(\\theta)_{22}\\) Continuing the calculation and remembering the commutation relation \\(\\sigma_i\\sigma_j = i\\epsilon_{ijk}\\sigma_k\\) \\[\\begin{align} R_z(\\theta)_{12} &amp;= \\dfrac 1 2 \\mathrm{tr}\\left(e^{-i\\theta \\sigma_3/2}\\sigma_1 e^{i\\theta \\sigma_3/2}\\sigma_2 \\right) \\\\ &amp;= \\dfrac 1 2 \\mathrm{tr}\\left(e^{-i\\theta \\sigma_3}\\sigma_1\\sigma_2 \\right) = \\dfrac 1 2 \\mathrm{tr}\\left(e^{-i\\theta \\sigma_3}i\\sigma_3\\right) \\\\ &amp;= \\dfrac{ie^{i\\theta} - ie^{-i\\theta}} 2 = \\dfrac{e^{-i\\theta}-e^{i\\theta}}{2i} = -\\sin\\theta \\end{align}\\] Then \\(R_z(\\theta)\\) restricted to the \\(xy\\) plane is a spatial rotation: \\[ R_z(\\theta)\\big|_{xy} = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} \\] We spend some time here to recall the hyperbolic functions. \\[\\begin{align} \\cosh x &amp;= \\sinh&#39;x = \\dfrac{e^x + e^{-x}} 2 \\\\ \\sinh x &amp;= \\cosh&#39;x = \\dfrac{e^x - e^{-x}} 2 \\\\ \\sinh(x+y) &amp;= \\sinh x \\cosh y + \\cosh x \\sinh y \\\\ \\cosh(x+y) &amp;= \\cosh x \\cosh y + \\sinh x \\sinh y \\\\ 1 &amp;= \\cosh^2 x - \\sinh^2 x \\end{align}\\] Theorem 5.3 (special Hermitians induce Lorentz boosts) Given \\(\\hat r\\in S^2, \\chi\\in \\mathbb R\\), the following Hermitian induces a Lorentz boost in the direction of \\(\\hat r\\in S^2\\) by \\(\\chi\\). \\[ H(\\hat r, \\chi) = \\exp \\left[-\\dfrac 1 2 \\chi (\\hat r\\cdot \\sigma)\\right]\\in \\mathcal H_2 \\] Proof: explicit calculation. Given \\(H(\\hat r, \\chi)\\), compute the matrix elements of the linear transform of \\(\\mathbb R^4\\) it corresponds to using equation (5.1); this turns out to be the Lorentz boost. "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
